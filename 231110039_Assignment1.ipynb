{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw9unZpeLqCh"
   },
   "source": [
    "**SOLUTION 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1708152820873,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "2H_slx1O6YSX"
   },
   "outputs": [],
   "source": [
    "# CLEANING HI_100_10k BEFORE TRAINING\n",
    "\n",
    "import re\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open('hi_100_10k.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.readlines()\n",
    "\n",
    "# Define a regular expression pattern to match English words\n",
    "english_pattern = re.compile(r'[a-zA-Z]+')\n",
    "\n",
    "# Iterate through each line in the corpus\n",
    "clean_corpus = []\n",
    "for line in corpus:\n",
    "    # Tokenize the line and remove English words\n",
    "    tokens = line.split()  # Assuming space-separated tokens\n",
    "    tokens = [re.sub(r'##', '', token) for token in tokens]  # Remove '##'\n",
    "    cleaned_line = ' '.join(tokens)\n",
    "    clean_corpus.append(cleaned_line)\n",
    "\n",
    "# Join the lines back into a single text\n",
    "clean_text = '\\n'.join(clean_corpus)\n",
    "\n",
    "# Print the updated content\n",
    "# print(clean_text)\n",
    "\n",
    "# Optionally, write the clean text back to a file\n",
    "with open('hi_100_10k.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(clean_text)\n",
    "\n",
    "# CLEANING THE FILE OVER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8QZWwQxB38Of"
   },
   "outputs": [],
   "source": [
    "# #CLEANING TEST.TXT BEFORE TRAINING\n",
    "\n",
    "# import re\n",
    "\n",
    "# # Read the contents of the text file\n",
    "# with open('test.txt', 'r', encoding='utf-8') as file:\n",
    "#     corpus = file.readlines()\n",
    "\n",
    "# # Define a regular expression pattern to match English words\n",
    "# english_pattern = re.compile(r'[a-zA-Z]+')\n",
    "\n",
    "# # Iterate through each line in the corpus\n",
    "# clean_corpus = []\n",
    "# for line in corpus:\n",
    "#     # Tokenize the line and remove English words\n",
    "#     tokens = line.split()  # Assuming space-separated tokens\n",
    "#     tokens = [token for token in tokens if not re.match(english_pattern, token)]\n",
    "#     cleaned_line = ' '.join(tokens)\n",
    "#     clean_corpus.append(cleaned_line)\n",
    "\n",
    "# # Join the lines back into a single text\n",
    "# clean_text = '\\n'.join(clean_corpus)\n",
    "\n",
    "# # Print the updated content\n",
    "# # print(clean_text)\n",
    "\n",
    "# # Optionally, write the clean text back to a file\n",
    "# with open('test.txt', 'w', encoding='utf-8') as file:\n",
    "#     file.write(clean_text)\n",
    "\n",
    "# #CLEANING THE FILE OVER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1708152835305,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "yFnlw37PLt1N"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define Arrays of various Devanagari characters, consonants, vowels, matras\n",
    "devanagari_chars = set(\"् अ अ आ अं  ं इ ई उ ऊ ए ऐ ओ औ ऋ क ख ग घ ङ च छ ज झ ञ ट ठ ड ढ ण त थ द ध न प फ ब भ म य र ल व श ष स ह ळ क्ष ज्ञ र्ड र्ढ ॐ । ॥ ा  ि  ी  ु  ू  ृ  ॄ  े  ै  ो  ौ  ॄ\".split())\n",
    "devanagari_consonants = set(\"क ख ग घ ङ च छ ज झ ञ ट ठ ड ढ ण त थ द ध न प फ ब भ म य र ल व श ष स ह क्ष ज्ञ\".split())\n",
    "devanagari_vowels = set(\"अ आ इ ई उ ऊ ऋ ए ऐ ओ औ\".split())\n",
    "devanagari_matras = set(\"ा  ि  ी  ु  ू  ृ  ॄ  े  ै  ो  ौ  ॄ  ं \".split())\n",
    "devanagari_vowel_matras = {\"ा\": \"आ\", \"ि\": \"इ\", \"ी\": \"ई\", \"ु\": \"उ\", \"ू\": \"ऊ\", \"े\": \"ए\", \"ै\": \"ऐ\", \"ो\": \"ओ\", \"ौ\": \"औ\", \"ं\": \"अं\", \"ृ\": \"ऋ\", \"ॉ\": \"ऑ\", \"ॄ\": \"ॠ\"}\n",
    "\n",
    "# Function to check if a word contains only Hindi characters\n",
    "def check_devanagri(word):\n",
    "    return all(char in devanagari_chars for char in word)\n",
    "\n",
    "corpus_words=[]\n",
    "# Open and read the text file\n",
    "with open('hi_100_10k.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    # Remove unwanted characters from the content\n",
    "    unwanted_chars = set(\"_ ० S ― = [ ] ...... । ; , : ! \\\" ? :- - { ( } ) :- . ॥ ” |\".split())\n",
    "    for char in unwanted_chars:\n",
    "        content = content.replace(char, '')\n",
    "    # Split the content into words\n",
    "    sample = content.split()\n",
    "\n",
    "    corpus_words=[word for word in sample if check_devanagri(word)]\n",
    "\n",
    "\n",
    "# Print the Devanagari words\n",
    "# print(corpus_words)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2UJTwyAdnT5"
   },
   "source": [
    "**CORRECTING THE UNICODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3162,
     "status": "ok",
     "timestamp": 1708152851709,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "VoMj34GPZyHR"
   },
   "outputs": [],
   "source": [
    "# Array which will store the corrected unicode corpus\n",
    "corrected_devanagari_unicodes = []\n",
    "\n",
    "# function to correct the unicode of words in courpus_words\n",
    "def module_unicode_correction(input):\n",
    "    next=0\n",
    "    corrected= [] #stores corrected unicode for this current word\n",
    "    for i in input:\n",
    "        next+=1\n",
    "\n",
    "        # handling vowel by simply adding to array\n",
    "        if(i in devanagari_vowels):\n",
    "            corrected.append(i)\n",
    "\n",
    "        # handling halanth or matra by ignoring as already handled while handling its prev character\n",
    "        elif(i=='्' or (i in devanagari_vowel_matras)): continue\n",
    "\n",
    "        #handling consonants\n",
    "        else:\n",
    "            corrected.append(i+'्')\n",
    "\n",
    "            #halanth follows consonant\n",
    "            if(next < len(input) and ((input[next]=='्'))):\n",
    "                continue\n",
    "\n",
    "            #matra follows consonant\n",
    "            elif(next < len(input) and ((input[next] in devanagari_matras))):\n",
    "                corrected.append(devanagari_vowel_matras[input[next]])\n",
    "\n",
    "            #consonant follows consonant or if it is the last char\n",
    "            else:\n",
    "                corrected.append('अ')\n",
    "    return corrected\n",
    "\n",
    "# word='प्रियांशु'\n",
    "# print(unicode_corrector(word))\n",
    "\n",
    "for i in corpus_words:\n",
    "    corrected_devanagari_unicodes.append(module_unicode_correction(i))\n",
    "\n",
    "# print(corrected_devanagari_unicodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P87atf8FsWyZ"
   },
   "source": [
    "**SOLUTION 2**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "PART 1: FINDING CHARACTERS OF EACH WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1708152858973,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "engdcq30PSWa"
   },
   "outputs": [],
   "source": [
    "# print(corrected_devanagari_unicodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6r61l0PtE95"
   },
   "source": [
    "PART 2: FINDING SYLLABLES OF EACH WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1708152872284,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "gJZaOVYjtufC"
   },
   "outputs": [],
   "source": [
    "# word=\"कल्पना\"\n",
    "\n",
    "def syllabify(input):\n",
    "    next=0\n",
    "    syl= []\n",
    "    for i in input:\n",
    "        next+=1 # next keeps track of what is the next character\n",
    "        str=\"\"  # string to be appended\n",
    "        # testing for the vowel\n",
    "        if(i in devanagari_vowels):\n",
    "            str=i\n",
    "        if(next-2>=0 and input[next-2]=='्'): continue\n",
    "        # matra or halanth\n",
    "        elif((i in devanagari_vowel_matras) or i=='्'): continue\n",
    "\n",
    "        #consonant followed by halanth\n",
    "        elif(next < len(input) and ((input[next]=='्')) and (next+2<len(input) and (input[next+2] in devanagari_matras))):\n",
    "            str=\"\"\n",
    "            str+=i+'्'+input[next+1]\n",
    "            # if(next+2<len(input) and (input[next+2] in devanagari_matras)):\n",
    "            str+=input[next+2]\n",
    "\n",
    "        elif(next < len(input) and ((input[next]=='्'))):\n",
    "          str=\"\"\n",
    "          str+=i+'्'+input[next+1]\n",
    "\n",
    "        #consonant followed by maatra\n",
    "        elif(next < len(input) and((input[next] in devanagari_matras))):\n",
    "            str=i+input[next]\n",
    "\n",
    "        #consonant followed by consonant or nothing\n",
    "        else:\n",
    "            str=i\n",
    "        syl.append(str)\n",
    "    return syl\n",
    "\n",
    "# for i in corpus_words:\n",
    "# print(syllabify(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njoldVxHI4sN"
   },
   "source": [
    "**PART 3 UNIGRAM BIGRAM FOR CHARACTERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2057,
     "status": "ok",
     "timestamp": 1708152884353,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "FdgCdZw8IOip",
    "outputId": "cf376505-1f0e-483d-c971-da8d79d12d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 6695834\n",
      "आ -> 2873764\n",
      "ए -> 2249302\n",
      "क् -> 2151678\n",
      "र् -> 2037302\n",
      "ई -> 1396801\n",
      "इ -> 1377637\n",
      "न् -> 1289519\n",
      "स् -> 1240321\n",
      "ह् -> 1109337\n",
      "म् -> 1022154\n",
      "त् -> 945398\n",
      "ल् -> 868639\n",
      "ओ -> 853251\n",
      "प् -> 761349\n",
      "य् -> 724212\n",
      "व् -> 600201\n",
      "द् -> 592844\n",
      "उ -> 563702\n",
      "ज् -> 520942\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 1140085\n",
      "अर् -> 751154\n",
      "क्अ -> 577987\n",
      "स्अ -> 498641\n",
      "न्अ -> 495448\n",
      "अन् -> 418768\n",
      "क्ए -> 404828\n",
      "अह् -> 382608\n",
      "प्अ -> 377062\n",
      "आर् -> 360978\n",
      "त्अ -> 342715\n",
      "अक् -> 335408\n",
      "न्ए -> 321154\n",
      "म्अ -> 311452\n",
      "क्आ -> 309961\n",
      "ल्अ -> 308307\n",
      "अत् -> 300686\n",
      "ह्ऐ -> 294895\n",
      "म्ए -> 293575\n",
      "य्आ -> 287764\n"
     ]
    }
   ],
   "source": [
    "unigram = [] # array which stores all the Unicode characters(unigrams)\n",
    "for word in corpus_words:\n",
    "  unigram.append(module_unicode_correction(word))\n",
    "\n",
    "\n",
    "\n",
    "count_map ={}\n",
    "\n",
    "for row in unigram:\n",
    "  for element in row:\n",
    "    if element in count_map:\n",
    "      count_map[element]+=1\n",
    "    else:\n",
    "      count_map[element]=1\n",
    "\n",
    "sorted_occurrences = sorted(count_map.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(min(20, len(sorted_occurrences))):\n",
    "    print(sorted_occurrences[i][0], \"->\", sorted_occurrences[i][1])\n",
    "\n",
    "\n",
    "\n",
    "# Counting the frequency of bigram characters\n",
    "\n",
    "# Initialize the bigram count map\n",
    "count_bmap = {}\n",
    "\n",
    "# Iterate over each row (list of characters) in the 2D array\n",
    "for word in unigram:\n",
    "    # Iterate over each pair of characters in the row\n",
    "    for i in range(len(word) - 1):\n",
    "        # Extract the current character and the next character to form a bigram\n",
    "        bigram = word[i] + word[i+1]\n",
    "\n",
    "        # Check if the bigram exists in the count_bmap\n",
    "        if bigram in count_bmap:\n",
    "            # If it exists, increment its count\n",
    "            count_bmap[bigram] += 1\n",
    "        else:\n",
    "            # If it doesn't exist, initialize its count to 1\n",
    "            count_bmap[bigram] = 1\n",
    "\n",
    "# Sort the bigram occurrences map by values in descending order\n",
    "sorted_boccurrences = sorted(count_bmap.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the first 20 most occurring bigrams\n",
    "print(\"First 20 most occurring bigrams:\")\n",
    "for i in range(min(20, len(sorted_boccurrences))):\n",
    "    print(sorted_boccurrences[i][0], \"->\", sorted_boccurrences[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZXatD-wNnTm"
   },
   "source": [
    "**PART 4 UNIGRAM BIGRAM FOR SYLLABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2010,
     "status": "ok",
     "timestamp": 1708152908823,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "wnrFuFaeNt_E",
    "outputId": "ab47ccd0-92b5-4be1-ed53-dee2abbaa252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 6695834\n",
      "आ -> 2873764\n",
      "ए -> 2249302\n",
      "क् -> 2151678\n",
      "र् -> 2037302\n",
      "ई -> 1396801\n",
      "इ -> 1377637\n",
      "न् -> 1289519\n",
      "स् -> 1240321\n",
      "ह् -> 1109337\n",
      "म् -> 1022154\n",
      "त् -> 945398\n",
      "ल् -> 868639\n",
      "ओ -> 853251\n",
      "प् -> 761349\n",
      "य् -> 724212\n",
      "व् -> 600201\n",
      "द् -> 592844\n",
      "उ -> 563702\n",
      "ज् -> 520942\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 1140085\n",
      "अर् -> 751154\n",
      "क्अ -> 577987\n",
      "स्अ -> 498641\n",
      "न्अ -> 495448\n",
      "अन् -> 418768\n",
      "क्ए -> 404828\n",
      "अह् -> 382608\n",
      "प्अ -> 377062\n",
      "आर् -> 360978\n",
      "त्अ -> 342715\n",
      "अक् -> 335408\n",
      "न्ए -> 321154\n",
      "म्अ -> 311452\n",
      "क्आ -> 309961\n",
      "ल्अ -> 308307\n",
      "अत् -> 300686\n",
      "ह्ऐ -> 294895\n",
      "म्ए -> 293575\n",
      "य्आ -> 287764\n"
     ]
    }
   ],
   "source": [
    "syllable_arr = []\n",
    "for word in corrected_devanagari_unicodes:\n",
    "  syllable_arr.append(syllabify(word))\n",
    "\n",
    "count_map ={}\n",
    "\n",
    "for row in syllable_arr:\n",
    "  for element in row:\n",
    "    if element in count_map:\n",
    "      count_map[element]+=1\n",
    "    else:\n",
    "      count_map[element]=1\n",
    "\n",
    "sorted_occurrences = sorted(count_map.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(min(20, len(sorted_occurrences))):\n",
    "    print(sorted_occurrences[i][0], \"->\", sorted_occurrences[i][1])\n",
    "\n",
    "\n",
    "count_bmap = {}\n",
    "\n",
    "# Iterate over each row (list of characters) in the 2D array\n",
    "for word in syllable_arr:\n",
    "    # Iterate over each pair of characters in the row\n",
    "    for i in range(len(word) - 1):\n",
    "        # Extract the current character and the next character to form a bigram\n",
    "        bigram = word[i] + word[i+1]\n",
    "\n",
    "        # Check if the bigram exists in the count_bmap\n",
    "        if bigram in count_bmap:\n",
    "            # If it exists, increment its count\n",
    "            count_bmap[bigram] += 1\n",
    "        else:\n",
    "            # If it doesn't exist, initialize its count to 1\n",
    "            count_bmap[bigram] = 1\n",
    "\n",
    "# Sort the bigram occurrences map by values in descending order\n",
    "sorted_boccurrences = sorted(count_bmap.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the first 20 most occurring bigrams\n",
    "print()\n",
    "print(\"First 20 most occurring bigrams:\")\n",
    "for i in range(min(20, len(sorted_boccurrences))):\n",
    "    print(sorted_boccurrences[i][0], \"->\", sorted_boccurrences[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIFG-Lpz8Il9"
   },
   "source": [
    "**SOLUTION 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-2bLAFU99Hg"
   },
   "source": [
    "UNIGRAM, V=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne5o2WAU4wj7"
   },
   "source": [
    "MODIFIED @ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10262,
     "status": "ok",
     "timestamp": 1708152926177,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "FaAegdM-4vI1",
    "outputId": "fbd57c31-327b-4639-c555-10b15859e5a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\yashwanth holla\\anaconda3\\lib\\site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8076,
     "status": "ok",
     "timestamp": 1708152936778,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "Au0IeWMAORWz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Unigram model, vocab size=1000\n",
    "%pip install sentencepiece\n",
    "input_file=\"hi_100_10k.txt\"\n",
    "\n",
    "import sentencepiece as sp\n",
    "sp.SentencePieceTrainer.train(input=input_file, model_prefix=\"unigram_1k\", vocab_size=1000, model_type='unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1391,
     "status": "ok",
     "timestamp": 1708152965841,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "EScnv8208ss6"
   },
   "outputs": [],
   "source": [
    "#Testing the model on a smaller corpus\n",
    "\n",
    "s = sp.SentencePieceProcessor()\n",
    "s.load(\"unigram_1k.model\")\n",
    "\n",
    "# Open the input file and read its contents\n",
    "with open(\"hi_100_10k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the input text\n",
    "token_array = s.encode(text, out_type=str)\n",
    "\n",
    "# Write the tokenized output to the output file\n",
    "with open(\"unigram_output_1k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(token_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPxB-H47-CeU"
   },
   "source": [
    "UNIGRAM, V=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6064,
     "status": "ok",
     "timestamp": 1708153018904,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "MsWy7sjx915D"
   },
   "outputs": [],
   "source": [
    "#Unigram model, vocab size=2000\n",
    "\n",
    "input_file=\"hi_100_10k.txt\"\n",
    "\n",
    "import sentencepiece as sp\n",
    "sp.SentencePieceTrainer.train(input=input_file, model_prefix=\"unigram_2k\", vocab_size=2000, model_type='unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2644,
     "status": "ok",
     "timestamp": 1708153038888,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "LP9G6vgE92DR"
   },
   "outputs": [],
   "source": [
    "#Testing the model on a smaller corpus\n",
    "\n",
    "s = sp.SentencePieceProcessor()\n",
    "s.load(\"unigram_2k.model\")\n",
    "\n",
    "# Open the input file and read its contents\n",
    "with open(\"hi_100_10k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the input text\n",
    "token_array2 = s.encode(text, out_type=str)\n",
    "\n",
    "# Write the tokenized output to the output file\n",
    "with open(\"unigram_output_2k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(token_array2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMz4Cy9N-wuF"
   },
   "source": [
    "TOKENIZED FILES UNICODDE CORRECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3265,
     "status": "ok",
     "timestamp": 1708153077717,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "j5U0PHOn-2wE"
   },
   "outputs": [],
   "source": [
    "# Array which will store the corrected unicode corpus\n",
    "\n",
    "\n",
    "# function to correct the unicode of words in courpus_words\n",
    "def module_unicode_correction_token(input):\n",
    "  next=0\n",
    "  corrected= [] #stores corrected unicode for this current word\n",
    "  if(len(input)==0): return corrected\n",
    "  if(len(input)==1):\n",
    "    if(input=='्' or (input in devanagari_matras)):\n",
    "      corrected.append(input)\n",
    "      return corrected\n",
    "  if(input[0] in devanagari_matras or input[0]=='्'):\n",
    "        corrected.append(input[0])\n",
    "\n",
    "  for i in input:\n",
    "      next+=1\n",
    "\n",
    "      if(i=='▁' or (i>='0' and i<='9')): continue\n",
    "\n",
    "      # handling vowel by simply adding to array\n",
    "      elif(i in devanagari_vowels):\n",
    "          corrected.append(i)\n",
    "\n",
    "      # handling halanth or matra by ignoring as already handled while handling its prev character\n",
    "      elif(i=='्' or (i in devanagari_vowel_matras)): continue\n",
    "\n",
    "      #handling consonants\n",
    "      else:\n",
    "          corrected.append(i+'्')\n",
    "\n",
    "          #halanth follows consonant\n",
    "          if(next < len(input) and ((input[next]=='्'))):\n",
    "              continue\n",
    "\n",
    "          #matra follows consonant\n",
    "          elif(next < len(input) and ((input[next] in devanagari_matras))):\n",
    "              corrected.append(devanagari_vowel_matras[input[next]])\n",
    "\n",
    "          #consonant follows consonant or if it is the last char\n",
    "          else:\n",
    "              corrected.append('अ')\n",
    "  return corrected\n",
    "\n",
    "#word='प्रियांशु'\n",
    "#print(unicode_corrector(word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_devanagari_tokens_1k = []\n",
    "with open(\"unigram_output_1k.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the content\n",
    "    content = file.read()\n",
    "    # Tokenize the content\n",
    "    tokens = content.split()  # Assuming tokens are separated by whitespace\n",
    "    # Correct the tokens and store them in the array\n",
    "    for token in tokens:\n",
    "        corrected_tokens = module_unicode_correction_token(token)\n",
    "        corrected_devanagari_tokens_1k.append(corrected_tokens)\n",
    "\n",
    "\n",
    "# print(corrected_devanagari_tokens_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708153089237,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "LXRxTrgLDu0q"
   },
   "outputs": [],
   "source": [
    "def token_syllabify(input):\n",
    "    next=0\n",
    "    syl= []\n",
    "\n",
    "    for i in input:\n",
    "        next+=1 # next keeps track of what is the next character\n",
    "        str=\"\"  # string to be appended\n",
    "        # testing for the vowel\n",
    "        if(i in devanagari_vowels):\n",
    "            str=i\n",
    "        if(next-2>=0 and input[next-2]=='्'): continue\n",
    "        # matra or halanth\n",
    "        elif((i in devanagari_vowel_matras) or i=='्' or i=='▁' or i=='।'): continue\n",
    "\n",
    "        #consonant followed by halanth\n",
    "        elif(next < len(input) and next<len(input) and ((input[next]=='्')) and (next+2<len(input) and (input[next+2] in devanagari_matras))):\n",
    "            str=\"\"\n",
    "            if(next+1<len(input)): str+=i+'्'+input[next+1]\n",
    "            # if(next+2<len(input) and (input[next+2] in devanagari_matras)):\n",
    "            str+=input[next+2]\n",
    "\n",
    "        elif(next < len(input) and ((input[next]=='्'))):\n",
    "          str=\"\"\n",
    "          str+=i+'्'\n",
    "          if(next+1<len(input)):\n",
    "            str+=input[next+1]\n",
    "\n",
    "        #consonant followed by maatra\n",
    "        elif(next < len(input) and((input[next] in devanagari_matras))):\n",
    "            str=i+input[next]\n",
    "\n",
    "        #consonant followed by consonant or nothing\n",
    "        else:\n",
    "            str=i\n",
    "        syl.append(str)\n",
    "    return syl\n",
    "\n",
    "# for i in corrected_devanagari_tokens_1k:\n",
    "#    print(token_syllabify(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU9LqQ6jFBEC"
   },
   "source": [
    "COUNTING FREQUENCIES FOR CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1708153097509,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "4pzEIMKpFF78"
   },
   "outputs": [],
   "source": [
    "\n",
    "def character_counter(token_array):\n",
    "  unigram = [] # array which stores all the Unicode characters(unigrams)\n",
    "  for word in token_array:\n",
    "    unigram.append(module_unicode_correction_token(word))\n",
    "\n",
    "\n",
    "  count_map ={}\n",
    "\n",
    "  for row in unigram:\n",
    "    for element in row:\n",
    "      if element in count_map:\n",
    "        count_map[element]+=1\n",
    "      else:\n",
    "        count_map[element]=1\n",
    "\n",
    "  sorted_occurrences = sorted(count_map.items(), key=lambda x: x[1], reverse=True)\n",
    "  for i in range(min(20, len(sorted_occurrences))):\n",
    "      print(sorted_occurrences[i][0], \"->\", sorted_occurrences[i][1])\n",
    "\n",
    "\n",
    "\n",
    "  # Counting the frequency of bigram characters\n",
    "\n",
    "  # Initialize the bigram count map\n",
    "  count_bmap = {}\n",
    "\n",
    "  # Iterate over each row (list of characters) in the 2D array\n",
    "  for word in unigram:\n",
    "      # Iterate over each pair of characters in the row\n",
    "      for i in range(len(word) - 1):\n",
    "          # Extract the current character and the next character to form a bigram\n",
    "          bigram = word[i] + word[i+1]\n",
    "\n",
    "          # Check if the bigram exists in the count_bmap\n",
    "          if bigram in count_bmap:\n",
    "              # If it exists, increment its count\n",
    "              count_bmap[bigram] += 1\n",
    "          else:\n",
    "              # If it doesn't exist, initialize its count to 1\n",
    "              count_bmap[bigram] = 1\n",
    "\n",
    "  # Sort the bigram occurrences map by values in descending order\n",
    "  sorted_boccurrences = sorted(count_bmap.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Print the first 20 most occurring bigrams\n",
    "  print(\"First 20 most occurring bigrams:\")\n",
    "  for i in range(min(20, len(sorted_boccurrences))):\n",
    "    print(sorted_boccurrences[i][0], \"->\", sorted_boccurrences[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V099UOcGmVc"
   },
   "source": [
    "COUNTING FREQUENCIES FOR SYLLABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708153101930,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "NxS5xRLpG8OL"
   },
   "outputs": [],
   "source": [
    "def syllable_counter(token_array):\n",
    "  syllable_arr = []\n",
    "  for word in token_array:\n",
    "    syllable_arr.append(token_syllabify(word))\n",
    "\n",
    "  count_map ={}\n",
    "\n",
    "  for row in syllable_arr:\n",
    "    for element in row:\n",
    "      if element in count_map:\n",
    "        count_map[element]+=1\n",
    "      else:\n",
    "        count_map[element]=1\n",
    "\n",
    "  sorted_occurrences = sorted(count_map.items(), key=lambda x: x[1], reverse=True)\n",
    "  for i in range(min(20, len(sorted_occurrences))):\n",
    "      print(sorted_occurrences[i][0], \"->\", sorted_occurrences[i][1])\n",
    "\n",
    "\n",
    "  count_bmap = {}\n",
    "\n",
    "  # Iterate over each row (list of characters) in the 2D array\n",
    "  for word in syllable_arr:\n",
    "      # Iterate over each pair of characters in the row\n",
    "      for i in range(len(word) - 1):\n",
    "          # Extract the current character and the next character to form a bigram\n",
    "          bigram = word[i] + word[i+1]\n",
    "\n",
    "          # Check if the bigram exists in the count_bmap\n",
    "          if bigram in count_bmap:\n",
    "              # If it exists, increment its count\n",
    "              count_bmap[bigram] += 1\n",
    "          else:\n",
    "              # If it doesn't exist, initialize its count to 1\n",
    "              count_bmap[bigram] = 1\n",
    "\n",
    "  # Sort the bigram occurrences map by values in descending order\n",
    "  sorted_boccurrences = sorted(count_bmap.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Print the first 20 most occurring bigrams\n",
    "  print()\n",
    "  print(\"First 20 most occurring bigrams:\")\n",
    "  for i in range(min(20, len(sorted_boccurrences))):\n",
    "      print(sorted_boccurrences[i][0], \"->\", sorted_boccurrences[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9uNMftZH-O1"
   },
   "source": [
    "CREATING ARRAY OF TOKENS FROM TOKEN FILE, V=1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1708153122795,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "dDatLW-1IHG0"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty array to store tokens\n",
    "token_array = []\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(\"unigram_output_1k.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the content of the file\n",
    "    data = file.read()\n",
    "    # Split the content by spaces to extract tokens\n",
    "    for char in unwanted_chars:\n",
    "        data = data.replace(char, '')\n",
    "    token_array = data.split()\n",
    "\n",
    "\n",
    "# Replace the character \"▁\" with an empty string in each word of token_array\n",
    "token_array = [word.replace(\"▁\", \"\") for word in token_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTvLRss1Jnkk"
   },
   "source": [
    "COUNTING FREQUENCIES FOR TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1708153125864,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "xOCJm5uPJdvs"
   },
   "outputs": [],
   "source": [
    "def token_counter(token_array):\n",
    "    count_map = {}\n",
    "    # Iterate over each string in the array and update the counts in the dictionary\n",
    "    for word in token_array:\n",
    "        # If the word is already in the dictionary, increment its count\n",
    "        if word in count_map:\n",
    "            count_map[word] += 1\n",
    "        # If the word is not in the dictionary, add it with a count of 1\n",
    "        else:\n",
    "            count_map[word] = 1\n",
    "\n",
    "    # Sort the dictionary by values in descending order\n",
    "    sorted_count_map = sorted(count_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the top 20 unigram frequencies\n",
    "    print(\"Top 20 unigram frequencies of tokens:\")\n",
    "    for word, count in sorted_count_map[:20]:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    count_bmap = {}\n",
    "\n",
    "    # Iterate over each pair of consecutive strings in the token_array list and update the counts in the dictionary\n",
    "    for i in range(len(token_array) - 1):\n",
    "        # Create a tuple representing the bigram\n",
    "        bigram = (token_array[i], token_array[i + 1])\n",
    "\n",
    "        # If the bigram is already in the dictionary, increment its count\n",
    "        if bigram in count_bmap:\n",
    "            count_bmap[bigram] += 1\n",
    "        # If the bigram is not in the dictionary, add it with a count of 1\n",
    "        else:\n",
    "            count_bmap[bigram] = 1\n",
    "\n",
    "    # Sort the dictionary by values in descending order\n",
    "    sorted_count_bmap = sorted(count_bmap.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the top 20 bigram frequencies\n",
    "    print(\"Top 20 bigram frequencies of tokens:\")\n",
    "    for bigram, count in sorted_count_bmap[:20]:\n",
    "        print(f\"{bigram}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6CPGhE7REuR"
   },
   "source": [
    " FOR VOCAB SIZE: 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3360,
     "status": "ok",
     "timestamp": 1708153149887,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "Fsor68FFRJah"
   },
   "outputs": [],
   "source": [
    "# Array which will store the corrected tokens\n",
    "corrected_tokens_unigram_2k = []\n",
    "\n",
    "token_array2=[]\n",
    "with open(\"unigram_output_2k.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    for char in unwanted_chars:\n",
    "        data = data.replace(char, '')\n",
    "    token_array2 = data.split()\n",
    "    for token in token_array2:\n",
    "        tokens = module_unicode_correction_token(token)\n",
    "        corrected_tokens_unigram_2k.append(tokens)\n",
    "# Replace the character \"▁\" with an empty string in each word of token_array\n",
    "token_array2 = [word.replace(\"▁\", \"\") for word in token_array2]\n",
    "\n",
    "# print(corrected_tokens_unigram_2k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHaAeBsOLtqh"
   },
   "source": [
    "FUNCTION CALL TO COUNT CHARACTERS, V=1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3023,
     "status": "ok",
     "timestamp": 1708153165015,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "t48J3qj4LsSk",
    "outputId": "d5ae63f9-5e19-4d26-cd92-e70eb162184e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 9899416\n",
      "आ -> 2434704\n",
      "क् -> 2220998\n",
      "र् -> 2115451\n",
      "ए -> 2027413\n",
      "न् -> 1334322\n",
      "स् -> 1283598\n",
      "ई -> 1235823\n",
      "ह् -> 1133109\n",
      "म् -> 1053158\n",
      "इ -> 1051089\n",
      "त् -> 980008\n",
      "ल् -> 919858\n",
      "प् -> 805828\n",
      "य् -> 753163\n",
      "ओ -> 639271\n",
      "व् -> 624678\n",
      "द् -> 607586\n",
      "ज् -> 556551\n",
      "ा -> 556194\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 1527618\n",
      "क्अ -> 710411\n",
      "न्अ -> 646330\n",
      "स्अ -> 636382\n",
      "प्अ -> 493754\n",
      "अर् -> 492241\n",
      "म्अ -> 488695\n",
      "त्अ -> 485237\n",
      "ल्अ -> 481065\n",
      "क्ए -> 400503\n",
      "य्अ -> 360754\n",
      "ह्अ -> 356977\n",
      "अह् -> 346025\n",
      "न्ए -> 323621\n",
      "ब्अ -> 317534\n",
      "ग्अ -> 315208\n",
      "ज्अ -> 314367\n",
      "व्अ -> 307133\n",
      "क्आ -> 298489\n",
      "ह्ऐ -> 296118\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(token_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VUcW9zcLzaq"
   },
   "source": [
    "FUNCTION CALL TO COUNT SYLLABLES, V=1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1969,
     "status": "ok",
     "timestamp": 1708153180193,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "1llPBNNoL7j7",
    "outputId": "4360aa9f-19f7-48aa-b0f0-1a04f9102622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "र -> 1275891\n",
      "क -> 698597\n",
      "न -> 638121\n",
      "स -> 601491\n",
      "प -> 481223\n",
      "ल -> 464062\n",
      "म -> 445679\n",
      "त -> 410397\n",
      "के -> 400503\n",
      "ह -> 356977\n",
      "ने -> 323621\n",
      "ब -> 313090\n",
      "ग -> 307697\n",
      "ज -> 302679\n",
      "का -> 298489\n",
      "है -> 296118\n",
      "द -> 276874\n",
      "ए -> 275223\n",
      "मे -> 267778\n",
      "व -> 257475\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 172941\n",
      "और -> 115376\n",
      "पर -> 112824\n",
      "इस -> 92876\n",
      "एक -> 58881\n",
      "लिए -> 53135\n",
      "ड़ -> 49847\n",
      "नही -> 47279\n",
      "कार -> 42589\n",
      "अप -> 39579\n",
      "किया -> 36573\n",
      "सम -> 36400\n",
      "यह -> 35278\n",
      "कहा -> 32993\n",
      "गया -> 30203\n",
      "रने -> 29867\n",
      "उन -> 29732\n",
      "आप -> 29224\n",
      "उस -> 27912\n",
      "साथ -> 27696\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllables\n",
    "\n",
    "syllable_counter(token_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4hCJQNFL2-k"
   },
   "source": [
    "FUNCTION CALL TO COUNT TOKENS, V=1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1708153185553,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "Uuw0Wtm3L6Zh",
    "outputId": "3ddb3b9c-2d4c-48e4-fce6-bfb0e4f1f75b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "ा: 478146\n",
      "र: 434848\n",
      "के: 349826\n",
      "न: 325993\n",
      "ल: 276679\n",
      "म: 259506\n",
      "में: 240292\n",
      "की: 220072\n",
      "ने: 216182\n",
      "है: 215661\n",
      "स: 212618\n",
      "क: 211055\n",
      "ी: 209757\n",
      "त: 195932\n",
      "का: 181126\n",
      "प: 178707\n",
      "े: 177683\n",
      "से: 174094\n",
      "को: 168433\n",
      "ज: 165745\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('के', 'लिए'): 43621\n",
      "('ा', 'र'): 28374\n",
      "('है', 'कि'): 25651\n",
      "('ों', 'के'): 21700\n",
      "('ा', 'न'): 21568\n",
      "('म', 'ा'): 21272\n",
      "('र', 'ा'): 19932\n",
      "('ों', 'में'): 19868\n",
      "('स', 'ा'): 19683\n",
      "('के', 'साथ'): 18485\n",
      "('ने', 'के'): 17464\n",
      "('ज', 'ा'): 17063\n",
      "('ों', 'को'): 16902\n",
      "('ता', 'है'): 16182\n",
      "('कहा', 'कि'): 16114\n",
      "('र', 'ी'): 15282\n",
      "('के', 'बाद'): 14512\n",
      "('ा', 'ने'): 14287\n",
      "('ा', 'म'): 14240\n",
      "('ते', 'हैं'): 14224\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of tokens\n",
    "\n",
    "token_counter(token_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNB61lNuSJAC"
   },
   "source": [
    "FUNCTION CALL TO COUNT CHARACTERS, V=2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3285,
     "status": "ok",
     "timestamp": 1708153202640,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "gvgnd7F3SMg2",
    "outputId": "192e91ee-ab15-4186-800a-49c8e3b1b050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 8858111\n",
      "आ -> 2768907\n",
      "क् -> 2220998\n",
      "ए -> 2155987\n",
      "र् -> 2115451\n",
      "न् -> 1334322\n",
      "स् -> 1283598\n",
      "ई -> 1280310\n",
      "इ -> 1176572\n",
      "ह् -> 1133109\n",
      "म् -> 1053158\n",
      "त् -> 980008\n",
      "ल् -> 919858\n",
      "प् -> 805828\n",
      "य् -> 753163\n",
      "ओ -> 716506\n",
      "व् -> 624678\n",
      "द् -> 607586\n",
      "ज् -> 556551\n",
      "उ -> 526236\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 1384886\n",
      "क्अ -> 686005\n",
      "अर् -> 629744\n",
      "न्अ -> 611920\n",
      "स्अ -> 584463\n",
      "प्अ -> 457034\n",
      "त्अ -> 432254\n",
      "ल्अ -> 429044\n",
      "क्ए -> 400927\n",
      "म्अ -> 393180\n",
      "अह् -> 356669\n",
      "ह्अ -> 327526\n",
      "न्ए -> 322155\n",
      "य्अ -> 321745\n",
      "क्आ -> 306800\n",
      "ह्ऐ -> 296102\n",
      "आर् -> 295443\n",
      "म्ए -> 291083\n",
      "य्आ -> 289024\n",
      "अन् -> 282837\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(token_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V6YO8egSRD_"
   },
   "source": [
    "FUNCTION CALL TO COUNT SYLLABLES, V=2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2396,
     "status": "ok",
     "timestamp": 1708153214715,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "3ManP2-9S6P5",
    "outputId": "78c8a9a5-3228-4129-95af-c3c7cc5e8c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "र -> 1153075\n",
      "क -> 668435\n",
      "न -> 600420\n",
      "स -> 550884\n",
      "प -> 435910\n",
      "ल -> 415007\n",
      "के -> 400228\n",
      "त -> 366789\n",
      "म -> 352196\n",
      "ने -> 322155\n",
      "ह -> 319756\n",
      "का -> 305866\n",
      "है -> 296102\n",
      "मे -> 288978\n",
      "ब -> 276577\n",
      "ए -> 275197\n",
      "ग -> 268999\n",
      "ज -> 264638\n",
      "अ -> 244502\n",
      "व -> 234827\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 171135\n",
      "और -> 115376\n",
      "पर -> 112130\n",
      "इस -> 84414\n",
      "एक -> 57179\n",
      "लिए -> 53135\n",
      "कार -> 49355\n",
      "नही -> 49103\n",
      "ड़ -> 48661\n",
      "अप -> 44765\n",
      "किया -> 36573\n",
      "यह -> 34020\n",
      "कहा -> 32991\n",
      "देश -> 30563\n",
      "रने -> 30559\n",
      "गया -> 30203\n",
      "आप -> 28672\n",
      "उन -> 28521\n",
      "उस -> 28345\n",
      "साथ -> 27696\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllables\n",
    "\n",
    "syllable_counter(token_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rJCyn_kSWnr"
   },
   "source": [
    "FUNCTION CALL TO COUNT TOKENS, V=2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1708153226148,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "CP6zXvsRUcrJ",
    "outputId": "0412cc4e-eb36-4735-f7dd-832c3219ac39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "के: 336021\n",
      "में: 240292\n",
      "है: 215652\n",
      "की: 206996\n",
      "न: 177273\n",
      "ने: 170640\n",
      "ी: 168064\n",
      "को: 161734\n",
      "से: 159010\n",
      "र: 158988\n",
      "का: 157826\n",
      "ल: 157351\n",
      "क: 138034\n",
      "म: 127023\n",
      ": 124511\n",
      "स: 123495\n",
      "और: 115376\n",
      "ा: 114518\n",
      "पर: 110185\n",
      "ों: 109878\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('के', 'लिए'): 43622\n",
      "('है', 'कि'): 25511\n",
      "('ों', 'के'): 19428\n",
      "('के', 'साथ'): 18476\n",
      "('ों', 'में'): 17906\n",
      "('कहा', 'कि'): 16109\n",
      "('ों', 'को'): 14967\n",
      "('के', 'बाद'): 14510\n",
      "('ों', 'की'): 12683\n",
      "('ने', 'कहा'): 12227\n",
      "('रहा', 'है'): 12023\n",
      "('ने', 'के'): 11704\n",
      "('है', 'और'): 11214\n",
      "('गया', 'है'): 11060\n",
      "('रहे', 'हैं'): 10179\n",
      "('करने', 'के'): 8848\n",
      "('है', ''): 8797\n",
      "('रही', 'है'): 8754\n",
      "('', ''): 8219\n",
      "('ता', 'है'): 8154\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of tokens\n",
    "\n",
    "token_counter(token_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2rUJHx1wafI"
   },
   "source": [
    "**BPE TOKENIZER STARTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaPrO1IvjIIi"
   },
   "source": [
    "BPE, VOCAB SIZE=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1708154318139,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "d3N20xG2ipkM"
   },
   "outputs": [],
   "source": [
    "#BPE model, vocab size=1000\n",
    "\n",
    "input_file=\"hi_100_10k.txt\"\n",
    "\n",
    "import sentencepiece as sp\n",
    "sp.SentencePieceTrainer.train(input=input_file, model_prefix=\"bpe_1k\", vocab_size=1000, model_type='bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2696,
     "status": "ok",
     "timestamp": 1708154358465,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "UlwnnrIJjLcB"
   },
   "outputs": [],
   "source": [
    "#Testing the model on a smaller corpus\n",
    "\n",
    "s = sp.SentencePieceProcessor()\n",
    "s.load(\"bpe_1k.model\")\n",
    "\n",
    "# Open the input file and read its contents\n",
    "with open(\"hi_100_10k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the input text\n",
    "token_array = s.encode(text, out_type=str)\n",
    "\n",
    "# Write the tokenized output to the output file\n",
    "with open(\"bpe_output_1k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(token_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UquByUJlluy"
   },
   "source": [
    "TOKENIZED FILES UNICODE CORRECTION, VOLUME = 1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 2700,
     "status": "ok",
     "timestamp": 1708154391628,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "5rSMS7jYlo2f"
   },
   "outputs": [],
   "source": [
    "corrected_devanagari_tokens_1k=[]\n",
    "\n",
    "with open(\"bpe_output_1k.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the content\n",
    "    content = file.read()\n",
    "    # Tokenize the content\n",
    "    token_array = content.split()  # Assuming tokens are separated by whitespace\n",
    "    # Correct the tokens and store them in the array\n",
    "    for token in token_array:\n",
    "        tokens = module_unicode_correction_token(token)\n",
    "        corrected_devanagari_tokens_1k.append(tokens)\n",
    "\n",
    "\n",
    "# print(corrected_devanagari_tokens_1k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLY_zKr7xSqH"
   },
   "source": [
    "CONVERTING TOKENS FROM FILE TO TOKENS IN ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1708154404702,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "68ef_KlexQU8"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty array to store tokens\n",
    "token_array = []\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(\"bpe_output_1k.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the content of the file\n",
    "    data = file.read()\n",
    "    # Split the content by spaces to extract tokens\n",
    "    for char in unwanted_chars:\n",
    "        data = data.replace(char, '')\n",
    "    token_array = data.split()\n",
    "\n",
    "\n",
    "# Replace the character \"▁\" with an empty string in each word of token_array\n",
    "token_array = [word.replace(\"▁\", \"\") for word in token_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2426,
     "status": "ok",
     "timestamp": 1708154411235,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "4CqJPJYJyKBQ",
    "outputId": "2c72abfd-3dc9-411f-a6fa-474f53ecb367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 9828144\n",
      "आ -> 2495295\n",
      "क् -> 2220998\n",
      "र् -> 2115451\n",
      "ए -> 2056601\n",
      "ई -> 1342158\n",
      "न् -> 1334322\n",
      "स् -> 1283598\n",
      "ह् -> 1133109\n",
      "म् -> 1053158\n",
      "त् -> 980008\n",
      "इ -> 941618\n",
      "ल् -> 919858\n",
      "प् -> 805828\n",
      "य् -> 753163\n",
      "ओ -> 636698\n",
      "व् -> 624678\n",
      "द् -> 607586\n",
      "ज् -> 556551\n",
      "ब् -> 524575\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 1442527\n",
      "क्अ -> 731293\n",
      "न्अ -> 617238\n",
      "स्अ -> 595015\n",
      "अर् -> 557198\n",
      "प्अ -> 511988\n",
      "म्अ -> 508071\n",
      "ल्अ -> 461635\n",
      "त्अ -> 452101\n",
      "क्ए -> 393577\n",
      "ह्अ -> 362677\n",
      "य्अ -> 347466\n",
      "ग्अ -> 331481\n",
      "न्ए -> 325947\n",
      "ब्अ -> 321559\n",
      "अह् -> 317337\n",
      "व्अ -> 304047\n",
      "ज्अ -> 301428\n",
      "ह्ऐ -> 296118\n",
      "य्आ -> 288173\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(token_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2378,
     "status": "ok",
     "timestamp": 1708154419918,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "42Ue_zwQy1s8",
    "outputId": "1b53ae93-6534-48c2-9035-b5e4bd88d8db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "र -> 1198198\n",
      "क -> 725371\n",
      "न -> 611278\n",
      "स -> 569693\n",
      "प -> 506770\n",
      "म -> 460720\n",
      "ल -> 451524\n",
      "त -> 411554\n",
      "के -> 393577\n",
      "ह -> 358272\n",
      "ग -> 331481\n",
      "ने -> 325947\n",
      "ब -> 321559\n",
      "ज -> 297357\n",
      "है -> 296118\n",
      "का -> 287298\n",
      "ए -> 279053\n",
      "द -> 278389\n",
      "व -> 277252\n",
      "मे -> 274560\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 176025\n",
      "और -> 115365\n",
      "पर -> 112490\n",
      "इस -> 81210\n",
      "कार -> 60726\n",
      "एक -> 54352\n",
      "लिए -> 53777\n",
      "अप -> 47971\n",
      "नही -> 47279\n",
      "ड़ -> 45497\n",
      "सम -> 39598\n",
      "किया -> 36546\n",
      "तर -> 34431\n",
      "देश -> 34025\n",
      "सर -> 31429\n",
      "कहा -> 31235\n",
      "यह -> 31055\n",
      "गया -> 30203\n",
      "रने -> 29856\n",
      "आप -> 29453\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllable\n",
    "\n",
    "syllable_counter(token_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1708154474340,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "RXI-wNTS1h2G",
    "outputId": "c0f59346-0198-4ed7-961f-920a2c94b941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "के: 364425\n",
      "में: 252763\n",
      "न: 234688\n",
      "की: 227547\n",
      "क: 221778\n",
      "म: 219906\n",
      "है: 215519\n",
      "ने: 205149\n",
      "ल: 196225\n",
      "प: 195028\n",
      "त: 192529\n",
      "स: 180036\n",
      "से: 171974\n",
      "र: 171061\n",
      "को: 170947\n",
      "व: 169110\n",
      "ज: 167772\n",
      "का: 164069\n",
      "ब: 156623\n",
      "द: 154973\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('के', 'लिए'): 44391\n",
      "('है', 'कि'): 25726\n",
      "('के', 'साथ'): 18662\n",
      "('ों', 'के'): 18023\n",
      "('कहा', 'कि'): 16136\n",
      "('ों', 'में'): 16094\n",
      "('ते', 'हैं'): 15852\n",
      "('ने', 'के'): 15696\n",
      "('के', 'बाद'): 15556\n",
      "('ों', 'को'): 14289\n",
      "('ता', 'है'): 13224\n",
      "('ने', 'कहा'): 12400\n",
      "('रहा', 'है'): 12023\n",
      "('ों', 'की'): 11534\n",
      "('है', 'और'): 11214\n",
      "('गया', 'है'): 11060\n",
      "('रहे', 'हैं'): 10181\n",
      "('वार', 'को'): 9229\n",
      "('ने', 'की'): 9093\n",
      "('करने', 'के'): 8848\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of token\n",
    "\n",
    "token_counter(token_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCJrL-y42DLO"
   },
   "source": [
    "BPE, VOCAB SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1708154543189,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "d087j3yn2KoA"
   },
   "outputs": [],
   "source": [
    "#BPE model, vocab size=2000\n",
    "\n",
    "input_file=\"hi_100_10k.txt\"\n",
    "\n",
    "import sentencepiece as sp\n",
    "sp.SentencePieceTrainer.train(input=input_file, model_prefix=\"bpe_2k\", vocab_size=2000, model_type='bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 2649,
     "status": "ok",
     "timestamp": 1708154572466,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "g4ZgJL_v2SBr"
   },
   "outputs": [],
   "source": [
    "#Testing the model on a smaller corpus\n",
    "\n",
    "s = sp.SentencePieceProcessor()\n",
    "s.load(\"bpe_2k.model\")\n",
    "\n",
    "# Open the input file and read its contents\n",
    "with open(\"hi_100_10k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the input text\n",
    "token_array2 = s.encode(text, out_type=str)\n",
    "\n",
    "# Write the tokenized output to the output file\n",
    "with open(\"bpe_output_2k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(token_array2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIdH5H0C2xKr"
   },
   "source": [
    "TOKENIZED FILE UNICODE CORRECTION, VOL = 2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44224,
     "status": "ok",
     "timestamp": 1708154655373,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "eG0TjzDU2wdF",
    "outputId": "aae6c304-aff1-458c-9fc7-4881d6661b9d"
   },
   "outputs": [],
   "source": [
    "corrected_devanagari_tokens_2k=[]\n",
    "\n",
    "with open(\"bpe_output_2k.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the content\n",
    "    content = file.read()\n",
    "    # Tokenize the content\n",
    "    token_array2 = content.split()  # Assuming tokens are separated by whitespace\n",
    "    # Correct the tokens and store them in the array\n",
    "    for token in token_array2:\n",
    "        tokens = module_unicode_correction_token(token)\n",
    "        # print(tokens)\n",
    "        corrected_devanagari_tokens_2k.append(tokens)\n",
    "\n",
    "\n",
    "# print(corrected_devanagari_tokens_2k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8oy0xpk69RW"
   },
   "source": [
    "CONVERT TOKEN FILE TO TOKEN ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1708154737652,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "CnJUp4xC68fU"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty array to store tokens\n",
    "token_array2 = []\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(\"bpe_output_2k.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the content of the file\n",
    "    data = file.read()\n",
    "    # Split the content by spaces to extract tokens\n",
    "    for char in unwanted_chars:\n",
    "        data = data.replace(char, '')\n",
    "    token_array2 = data.split()\n",
    "\n",
    "\n",
    "# Replace the character \"▁\" with an empty string in each word of token_array\n",
    "token_array2 = [word.replace(\"▁\", \"\") for word in token_array2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8444,
     "status": "ok",
     "timestamp": 1708154753374,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "lu0ryeNl3_7N",
    "outputId": "b906526a-aa84-4a35-cd95-9c045e47d479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 8972205\n",
      "आ -> 2688032\n",
      "क् -> 2220998\n",
      "ए -> 2171626\n",
      "र् -> 2115451\n",
      "ई -> 1403945\n",
      "न् -> 1334322\n",
      "स् -> 1283598\n",
      "ह् -> 1133109\n",
      "इ -> 1103639\n",
      "म् -> 1053158\n",
      "त् -> 980008\n",
      "ल् -> 919858\n",
      "प् -> 805828\n",
      "य् -> 753163\n",
      "ओ -> 716010\n",
      "व् -> 624678\n",
      "द् -> 607586\n",
      "ज् -> 556551\n",
      "ब् -> 524575\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 1357137\n",
      "क्अ -> 681268\n",
      "अर् -> 635277\n",
      "न्अ -> 587899\n",
      "स्अ -> 577911\n",
      "प्अ -> 464140\n",
      "म्अ -> 428527\n",
      "ल्अ -> 426036\n",
      "त्अ -> 408978\n",
      "क्ए -> 402863\n",
      "अह् -> 349872\n",
      "ह्अ -> 337506\n",
      "न्ए -> 325947\n",
      "य्अ -> 325788\n",
      "क्आ -> 304294\n",
      "ब्अ -> 296993\n",
      "ह्ऐ -> 296118\n",
      "ग्अ -> 293109\n",
      "य्आ -> 291596\n",
      "आर् -> 289886\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(token_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1799,
     "status": "ok",
     "timestamp": 1708154761643,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "OSnZbbuS4cwV",
    "outputId": "67ddf879-533f-4132-e9a7-00007f2acfdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "र -> 1132900\n",
      "क -> 672208\n",
      "न -> 581939\n",
      "स -> 550856\n",
      "प -> 452426\n",
      "ल -> 415925\n",
      "के -> 402863\n",
      "म -> 381616\n",
      "त -> 364395\n",
      "ह -> 334846\n",
      "ने -> 325947\n",
      "का -> 302692\n",
      "ब -> 296993\n",
      "है -> 296118\n",
      "ग -> 291380\n",
      "मे -> 285739\n",
      "ए -> 279053\n",
      "ज -> 263527\n",
      "व -> 242247\n",
      "अ -> 239076\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 169576\n",
      "और -> 115365\n",
      "पर -> 107319\n",
      "इस -> 83160\n",
      "एक -> 54352\n",
      "लिए -> 53777\n",
      "कार -> 49903\n",
      "नही -> 49103\n",
      "अप -> 46073\n",
      "ड़ -> 43487\n",
      "किया -> 36546\n",
      "सम -> 35035\n",
      "सर -> 31429\n",
      "कहा -> 31235\n",
      "यह -> 31055\n",
      "गया -> 30203\n",
      "देश -> 29873\n",
      "रने -> 29856\n",
      "आप -> 29453\n",
      "तर -> 28358\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllable\n",
    "\n",
    "syllable_counter(token_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1474,
     "status": "ok",
     "timestamp": 1708154781748,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "TrmxkLmd4l04",
    "outputId": "d18abd87-69e4-4519-f88a-f2dae698ec73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "के: 344221\n",
      "में: 245492\n",
      "है: 215519\n",
      "की: 214643\n",
      "ने: 178498\n",
      "को: 164636\n",
      "से: 161950\n",
      "का: 147960\n",
      "क: 131671\n",
      "न: 122821\n",
      "और: 115365\n",
      "म: 109222\n",
      "स: 108686\n",
      "पर: 105519\n",
      "प: 97698\n",
      "ल: 94387\n",
      "कि: 92992\n",
      "व: 91544\n",
      "त: 91188\n",
      ": 89992\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('के', 'लिए'): 43665\n",
      "('है', 'कि'): 25487\n",
      "('के', 'साथ'): 18478\n",
      "('कहा', 'कि'): 16128\n",
      "('के', 'बाद'): 14524\n",
      "('ों', 'में'): 12934\n",
      "('ों', 'के'): 12918\n",
      "('ने', 'कहा'): 12055\n",
      "('रहा', 'है'): 12023\n",
      "('ने', 'के'): 11870\n",
      "('है', 'और'): 11214\n",
      "('गया', 'है'): 11060\n",
      "('रहे', 'हैं'): 10181\n",
      "('ों', 'को'): 10095\n",
      "('ता', 'है'): 9988\n",
      "('करने', 'के'): 8848\n",
      "('रही', 'है'): 8754\n",
      "('ों', 'की'): 8237\n",
      "('है', ''): 8092\n",
      "('', ''): 8013\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of token\n",
    "\n",
    "token_counter(token_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV-xgmWFIXIz"
   },
   "source": [
    "**BERT TOKENIZER STARTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8Q9EpE33igX"
   },
   "source": [
    "Volume = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc-RK_Sf7k3B"
   },
   "source": [
    "MODIFIED @ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7014,
     "status": "ok",
     "timestamp": 1708154812986,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "EJAvbDtd7iap",
    "outputId": "d64c3d95-1751-4d9a-89db-851d77f4a9eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.0-cp39-cp39-win_amd64.whl (198.5 MB)\n",
      "Collecting flax\n",
      "  Downloading flax-0.8.1-py3-none-any.whl (677 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.13.1)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from torch) (2024.2.0)\n",
      "Collecting msgpack\n",
      "  Downloading msgpack-1.0.7-cp39-cp39-win_amd64.whl (222 kB)\n",
      "Collecting tensorstore\n",
      "  Downloading tensorstore-0.1.53-cp39-cp39-win_amd64.whl (11.0 MB)\n",
      "Collecting orbax-checkpoint\n",
      "  Downloading orbax_checkpoint-0.5.3-py3-none-any.whl (143 kB)\n",
      "Collecting jax>=0.4.19\n",
      "  Downloading jax-0.4.24-py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from flax) (1.26.4)\n",
      "Collecting rich>=11.1\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from flax) (6.0.1)\n",
      "Collecting optax\n",
      "  Downloading optax-0.1.9-py3-none-any.whl (197 kB)\n",
      "Collecting tensorflow-intel==2.15.0\n",
      "  Downloading tensorflow_intel-2.15.0-cp39-cp39-win_amd64.whl (300.8 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: setuptools in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (57.4.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.1-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl (938 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.3-cp39-cp39-win_amd64.whl (413 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Collecting scipy>=1.9\n",
      "  Downloading scipy-1.12.0-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from jax>=0.4.19->flax) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from importlib-metadata>=4.6->jax>=0.4.19->flax) (3.17.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from rich>=11.1->flax) (2.17.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.28.0-py2.py3-none-any.whl (186 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.2)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting chex>=0.1.7\n",
      "  Downloading chex-0.1.85-py3-none-any.whl (95 kB)\n",
      "Collecting jaxlib>=0.1.37\n",
      "  Downloading jaxlib-0.4.23-cp39-cp39-win_amd64.whl (45.9 MB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch flax tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6478,
     "status": "ok",
     "timestamp": 1708156089921,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "IjA5wReXIWpi",
    "outputId": "d6a42565-6efb-4b27-9f3d-56f89ec7b98e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c66e88d3be49c3bfd5acab3d2a4660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yashwanth Holla\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Yashwanth Holla\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802ecf94366248d78558a3751b7ac315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7089aad760fd4c8ab7b9b1d5b1f62932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0400254d01734351adffbe03ae1b498d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer with maximum input length set to 1000\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_basic_tokenize=False, max_length=1000)\n",
    "\n",
    "# Path to your text file\n",
    "file_path = \"hi_100_10k.txt\"\n",
    "\n",
    "# Read text from the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize text using the BERT tokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert token IDs to tokens (characters)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"].squeeze())\n",
    "if(tokens[0] == 'CLS'):\n",
    "  tokens = tokens[1:]\n",
    "tokens = [token.lstrip('#') for token in tokens]\n",
    "# Print the tokens\n",
    "# print('\\n'.join(tokens))\n",
    "# print(type(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuBvcMVYK3Qe"
   },
   "source": [
    "PERFORMING UNICODE CORRECTION ON THIS TOKEN ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uhGTmKN4JnV6"
   },
   "outputs": [],
   "source": [
    "# IT IS NOT REQUIRED AS I AM ALREADY CORRECTING THE UNICODE INSIDE THE COUNTER FUNCTIONS\n",
    "\n",
    "\n",
    "# corrected_bert_token_unicode = []\n",
    "# for words in tokens:\n",
    "#   corrected_bert_token_unicode.append(module_unicode_correction_token(words))\n",
    "\n",
    "# print(corrected_bert_token_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3495,
     "status": "ok",
     "timestamp": 1708154928945,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "bfGa0ADigppt",
    "outputId": "0aa9e78c-b938-4dff-90ac-7015916725e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 11531653\n",
      "आ -> 2450332\n",
      "क् -> 2200621\n",
      "र् -> 2091650\n",
      "ए -> 2014913\n",
      "न् -> 1318683\n",
      "स् -> 1267313\n",
      "ई -> 1234877\n",
      "ह् -> 1124191\n",
      "म् -> 1040752\n",
      "त् -> 964377\n",
      "ल् -> 906445\n",
      "इ -> 892759\n",
      "प् -> 792217\n",
      "य् -> 740773\n",
      "् -> 650427\n",
      "व् -> 616126\n",
      "द् -> 600664\n",
      "ओ -> 588409\n",
      "ज् -> 544758\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 1615694\n",
      "क्अ -> 765373\n",
      "स्अ -> 753511\n",
      "न्अ -> 725397\n",
      "प्अ -> 600002\n",
      "अर् -> 592033\n",
      "म्अ -> 534360\n",
      "ल्अ -> 489125\n",
      "त्अ -> 487882\n",
      "क्ए -> 396355\n",
      "ब्अ -> 372186\n",
      "ह्अ -> 349385\n",
      "ग्अ -> 346357\n",
      "य्अ -> 332973\n",
      "व्अ -> 305563\n",
      "न्ए -> 303551\n",
      "क्आ -> 298313\n",
      "अन् -> 296778\n",
      "ह्ऐ -> 294543\n",
      "ज्अ -> 292206\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2102,
     "status": "ok",
     "timestamp": 1708154936610,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "SvVJMXL-g2vR",
    "outputId": "b4f940b6-e8d7-42d7-a1f8-6dab5a879bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "र -> 1401706\n",
      "क -> 735765\n",
      "स -> 715706\n",
      "न -> 710083\n",
      "प -> 595256\n",
      "म -> 496714\n",
      "ल -> 464133\n",
      "त -> 416982\n",
      "के -> 396355\n",
      "ब -> 371512\n",
      "ह -> 349157\n",
      "ग -> 337316\n",
      "ने -> 302934\n",
      "का -> 295267\n",
      "है -> 294543\n",
      "मे -> 276156\n",
      "ज -> 275525\n",
      "ए -> 270851\n",
      "द -> 268287\n",
      "व -> 250533\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 202581\n",
      "पर -> 127291\n",
      "और -> 115343\n",
      "UN -> 93973\n",
      "NK -> 93963\n",
      "[U -> 93962\n",
      "K] -> 93962\n",
      "इस -> 83193\n",
      "एक -> 58073\n",
      "लिए -> 53100\n",
      "नही -> 47192\n",
      "कार -> 44698\n",
      "अप -> 39566\n",
      "किया -> 36535\n",
      "यह -> 34013\n",
      "हर -> 33674\n",
      "ड़ -> 32812\n",
      "कहा -> 32788\n",
      "उन -> 30906\n",
      "गया -> 30157\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllable\n",
    "\n",
    "syllable_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1715,
     "status": "ok",
     "timestamp": 1708154943319,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "rI0g0GVng65I",
    "outputId": "b1b8c91d-41d6-4a29-a1ea-ee86f72a24b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "के: 346763\n",
      "स: 326183\n",
      "प: 306454\n",
      "म: 302469\n",
      "र: 295929\n",
      "।: 274507\n",
      "ब: 252891\n",
      "में: 246059\n",
      "न: 231845\n",
      "है: 214399\n",
      "की: 212603\n",
      "ा: 198455\n",
      "क: 188692\n",
      ",: 185278\n",
      "ल: 184653\n",
      "को: 173751\n",
      "ने: 170956\n",
      "ी: 169947\n",
      "से: 165288\n",
      "का: 163988\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('है', '।'): 91075\n",
      "('के', 'लिए'): 44010\n",
      "('प', '्र'): 37977\n",
      "('हैं', '।'): 34233\n",
      "('आ', 'प'): 26427\n",
      "('है', '.'): 26309\n",
      "('है', 'कि'): 25124\n",
      "('है', ','): 19971\n",
      "('.', '.'): 19711\n",
      "('के', 'साथ'): 18549\n",
      "('म', 'ु'): 17911\n",
      "('ों', 'के'): 16913\n",
      "('स', 'म'): 16477\n",
      "('स', 'ु'): 16181\n",
      "('कहा', 'कि'): 16102\n",
      "('ब', 'ता'): 15882\n",
      "('ों', 'में'): 15213\n",
      "('स', 'ा'): 14764\n",
      "('के', 'बाद'): 14506\n",
      "('ज', 'र'): 14488\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of token\n",
    "\n",
    "token_counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuKzpK-V3pUG"
   },
   "source": [
    "Volume = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (4.37.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: requests in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 52508,
     "status": "ok",
     "timestamp": 1708155111584,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "JpdhA87B3soO"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer with maximum input length set to 2000\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_basic_tokenize=False, max_length=2000)\n",
    "\n",
    "# Path to your text file\n",
    "file_path = \"hi_100_10kk.txt\"\n",
    "\n",
    "# Read text from the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize text using the BERT tokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert token IDs to tokens (characters)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"].squeeze())\n",
    "if(tokens[0] == 'CLS'):\n",
    "  tokens = tokens[1:]\n",
    "tokens = [token.lstrip('#') for token in tokens]\n",
    "\n",
    "\n",
    "#MODIFIED @ 3\n",
    "def is_number(s):\n",
    "  return s.isdigit()\n",
    "\n",
    "for i in tokens:\n",
    "    if is_number(i):\n",
    "        index_to_delete = tokens.index(i)\n",
    "        del tokens[index_to_delete]\n",
    "\n",
    "# Print the filtered array\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2714,
     "status": "ok",
     "timestamp": 1708155122683,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "JMNkIu-Z35eI",
    "outputId": "d67ce0d1-d312-478e-a345-b2b565d3c15b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 389431\n",
      "आ -> 82053\n",
      "क् -> 73828\n",
      "र् -> 69838\n",
      "ए -> 67473\n",
      "न् -> 44060\n",
      "स् -> 42122\n",
      "ई -> 41324\n",
      "ह् -> 37811\n",
      "म् -> 34347\n",
      "त् -> 32398\n",
      "ल् -> 30316\n",
      "इ -> 30050\n",
      "प् -> 26528\n",
      "य् -> 24795\n",
      "् -> 21331\n",
      "व् -> 20354\n",
      "ओ -> 19926\n",
      "द् -> 19693\n",
      "ज् -> 18214\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 53805\n",
      "क्अ -> 25696\n",
      "स्अ -> 25178\n",
      "न्अ -> 24247\n",
      "प्अ -> 20095\n",
      "अर् -> 19702\n",
      "म्अ -> 17490\n",
      "ल्अ -> 16311\n",
      "त्अ -> 16284\n",
      "क्ए -> 13397\n",
      "ब्अ -> 12149\n",
      "ह्अ -> 11584\n",
      "ग्अ -> 11479\n",
      "य्अ -> 11151\n",
      "न्ए -> 10149\n",
      "व्अ -> 10044\n",
      "ह्ऐ -> 10027\n",
      "अन् -> 9991\n",
      "ज्अ -> 9748\n",
      "क्आ -> 9718\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2641,
     "status": "ok",
     "timestamp": 1708155130731,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "VemlKKad39wt",
    "outputId": "19b676cf-3297-4fc3-ba40-5bd1926493f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "र -> 46806\n",
      "क -> 24650\n",
      "स -> 23935\n",
      "न -> 23748\n",
      "प -> 19937\n",
      "म -> 16319\n",
      "ल -> 15513\n",
      "त -> 13965\n",
      "के -> 13397\n",
      "ब -> 12126\n",
      "ह -> 11577\n",
      "ग -> 11186\n",
      "ने -> 10135\n",
      "है -> 10027\n",
      "का -> 9621\n",
      "मे -> 9255\n",
      "ज -> 9201\n",
      "ए -> 9137\n",
      "द -> 8737\n",
      "व -> 8223\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 6795\n",
      "पर -> 4232\n",
      "और -> 3891\n",
      "[U -> 3166\n",
      "UN -> 3166\n",
      "NK -> 3166\n",
      "K] -> 3166\n",
      "इस -> 2840\n",
      "एक -> 1972\n",
      "लिए -> 1831\n",
      "नही -> 1556\n",
      "कार -> 1492\n",
      "अप -> 1344\n",
      "किया -> 1255\n",
      "ड़ -> 1166\n",
      "यह -> 1150\n",
      "हर -> 1138\n",
      "कहा -> 1071\n",
      "उन -> 1054\n",
      "गया -> 1012\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllable\n",
    "\n",
    "syllable_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1159,
     "status": "ok",
     "timestamp": 1708155136600,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "0wYTT6iT3-un",
    "outputId": "0e11728a-eaaf-40a8-ece9-9ee0a95ced15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "के: 11669\n",
      "स: 10859\n",
      "प: 10225\n",
      "र: 9998\n",
      "म: 9686\n",
      "।: 9174\n",
      "में: 8281\n",
      "ब: 8260\n",
      "न: 7738\n",
      "है: 7283\n",
      "की: 7217\n",
      "ा: 6620\n",
      "ल: 6393\n",
      "क: 6383\n",
      ",: 6100\n",
      "को: 5867\n",
      "ने: 5729\n",
      "ी: 5703\n",
      "से: 5476\n",
      "का: 5264\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('है', '।'): 3064\n",
      "('के', 'लिए'): 1514\n",
      "('प', '्र'): 1274\n",
      "('हैं', '।'): 1144\n",
      "('है', '.'): 921\n",
      "('आ', 'प'): 901\n",
      "('है', 'कि'): 864\n",
      "('.', '.'): 766\n",
      "('है', ','): 655\n",
      "('के', 'साथ'): 652\n",
      "('ों', 'के'): 583\n",
      "('म', 'ु'): 543\n",
      "('स', 'ु'): 542\n",
      "('स', 'म'): 538\n",
      "('कहा', 'कि'): 528\n",
      "('ज', 'र'): 523\n",
      "('ब', 'ता'): 513\n",
      "('के', 'बाद'): 480\n",
      "('ों', 'में'): 479\n",
      "('स', 'ा'): 478\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllable\n",
    "\n",
    "token_counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6rP_xI8j_C4"
   },
   "source": [
    "**IndicBERT STARTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNTPXR082n9X"
   },
   "source": [
    "Volume = 1K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDhvmzTF8pvV"
   },
   "source": [
    "MODIFIED @ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "executionInfo": {
     "elapsed": 16591,
     "status": "ok",
     "timestamp": 1708155160078,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "YHhnAXHR8o8T",
    "outputId": "ea2266ae-de8e-49fd-885d-ba402153781c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (4.37.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\priyanshuj23\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uu4C0vc8zmj"
   },
   "source": [
    "MODIFIED @ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6141,
     "status": "ok",
     "timestamp": 1708155166215,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "UHguFo4D8yrL",
    "outputId": "1807cd5d-27c4-4683-c6db-0c4fffe16530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-4.25.3-cp39-cp39-win_amd64.whl (413 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-4.25.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "488a789c9bde41a3a9d01440f67decc5",
      "af8a40064d654ef99fa7b741e475b43e",
      "1ca515dfe5534b86af3824035f7f6239",
      "222d6a5624df4859855de6c3cfe7f834",
      "454a4782f1d44b179fa3f8ac69c77328",
      "68b8d34b478a4a5ab3558a7434cb7459",
      "63883d5281ab469c9a1ba43bbbd5b73e",
      "7b2957007e94420c8daaefd1e55dc229",
      "c438d69fd1904c979146ac383efa543f",
      "8c0bafabeb6d4ca5850baa4e14387741",
      "1d95cf017efa466692bebe5e4d9a79f9",
      "f8432510e8ce4bf696310dfc7ebf5267",
      "7ae36ca536c940139a04d95e84ac15d8",
      "27a718e90115457c91df941a5851c567",
      "5eb934c2341341a08f2e9a8972b21249",
      "cf2a9764fb364d599e58375103a9b5c1",
      "00b5b59737804b8b8e11b25506e1aa93",
      "f3f6c3122c1c4f33831356b2ba2d2d20",
      "91c8d5a0154b4b03b28a04c9676a0e35",
      "abe393450545466a8918234d4dc168d8",
      "89b110c7d1924ffe8cc643be8f02b8fb",
      "d5d00fdf0acc454595062d61ad9e0b39"
     ]
    },
    "executionInfo": {
     "elapsed": 6359,
     "status": "ok",
     "timestamp": 1708155206126,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "rogXOBBdj-ga",
    "outputId": "da6239cb-5c1e-4217-945f-cc9eb73f0e80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 507/507 [00:00<00:00, 127kB/s]\n",
      "C:\\Users\\priyanshuj23\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\priyanshuj23\\.cache\\huggingface\\hub\\models--ai4bharat--indic-bert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "spiece.model: 100%|██████████| 5.65M/5.65M [00:01<00:00, 3.86MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the IndicBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Adjust the max_model_input_sizes parameter to limit the vocabulary size\n",
    "tokenizer.max_model_input_sizes = {\"ai4bharat/indic-bert\": 1000}\n",
    "\n",
    "# Path to your text file\n",
    "file_path = \"hi_100_10k.txt\"\n",
    "\n",
    "# Read text from the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize text using the IndicBERT tokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "tokens = [string.replace(\"▁\", \"\") for string in tokens]\n",
    "\n",
    "# Remove numeric strings from the array\n",
    "tokens = [token for token in tokens if not re.match(r'^[0-9]+$', token)]\n",
    "\n",
    "\n",
    "# Print the tokens\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3429,
     "status": "ok",
     "timestamp": 1708155217160,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "ANe5JCUFxPk3",
    "outputId": "a5bdf25b-eb4a-44e2-9ab5-9bc03e510d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 19017320\n",
      "क् -> 2220998\n",
      "र् -> 2115732\n",
      "न् -> 1335136\n",
      "स् -> 1283598\n",
      "ह् -> 1133109\n",
      "म् -> 1053158\n",
      "त् -> 980008\n",
      "ल् -> 919858\n",
      "प् -> 805828\n",
      "य् -> 753163\n",
      "व् -> 624678\n",
      "द् -> 607586\n",
      "ज् -> 556551\n",
      "ब् -> 524575\n",
      "ग् -> 479916\n",
      "ट् -> 318582\n",
      "श् -> 297460\n",
      "ए -> 286303\n",
      "।् -> 276879\n",
      "First 20 most occurring bigrams:\n",
      "क्अ -> 2220998\n",
      "र्अ -> 2115732\n",
      "अर् -> 1539413\n",
      "न्अ -> 1335136\n",
      "स्अ -> 1283598\n",
      "ह्अ -> 1133109\n",
      "म्अ -> 1053158\n",
      "त्अ -> 980008\n",
      "ल्अ -> 919858\n",
      "प्अ -> 805828\n",
      "य्अ -> 753163\n",
      "अन् -> 694189\n",
      "व्अ -> 624678\n",
      "द्अ -> 607586\n",
      "ज्अ -> 556551\n",
      "अल् -> 526114\n",
      "ब्अ -> 524575\n",
      "ग्अ -> 479916\n",
      "अत् -> 422462\n",
      "अह् -> 410498\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3241,
     "status": "ok",
     "timestamp": 1708155229797,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "rW3dev1KxSir",
    "outputId": "b1e8dbc3-e63d-4bb1-a5c8-fabe074e0e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "क -> 2220998\n",
      "र -> 2115732\n",
      "न -> 1335136\n",
      "स -> 1283598\n",
      "ह -> 1133109\n",
      "म -> 1053158\n",
      "त -> 980008\n",
      "ल -> 919858\n",
      "प -> 805828\n",
      "य -> 753163\n",
      "व -> 624678\n",
      "द -> 607586\n",
      "ज -> 556551\n",
      "ब -> 524575\n",
      "ग -> 479916\n",
      "ट -> 318582\n",
      "श -> 297460\n",
      "ए -> 286303\n",
      "च -> 272528\n",
      "अ -> 253182\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 331417\n",
      "पर -> 311463\n",
      "और -> 115918\n",
      "इस -> 100938\n",
      "तर -> 94880\n",
      "रह -> 93236\n",
      "वर -> 91160\n",
      "दर -> 81144\n",
      "रत -> 81053\n",
      "सम -> 73028\n",
      "सर -> 69816\n",
      "गर -> 69613\n",
      "जन -> 67614\n",
      "लग -> 63925\n",
      "मन -> 62257\n",
      "एक -> 60679\n",
      "बर -> 59271\n",
      "पन -> 58530\n",
      "उन -> 57239\n",
      "मर -> 55870\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllable\n",
    "\n",
    "syllable_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1606,
     "status": "ok",
     "timestamp": 1708155236728,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "pNUz1RxWxWCv",
    "outputId": "ec9a0714-81fe-4d0a-8b4b-5185fbefada8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "क: 1148430\n",
      "ह: 528771\n",
      "य: 451165\n",
      "न: 415566\n",
      "स: 388035\n",
      "म: 358981\n",
      "त: 314370\n",
      "पर: 278825\n",
      "।: 274128\n",
      ": 255196\n",
      "कर: 208176\n",
      "द: 187335\n",
      ",: 181924\n",
      "ल: 171849\n",
      "व: 164840\n",
      "ग: 158211\n",
      "ज: 156321\n",
      "र: 146549\n",
      "थ: 139596\n",
      "ए: 136758\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('ह', '।'): 129657\n",
      "('न', 'ह'): 61184\n",
      "('त', 'ह'): 59911\n",
      "('क', 'य'): 58547\n",
      "('य', 'क'): 56876\n",
      "('ल', 'ए'): 54172\n",
      "('क', 'ल'): 48659\n",
      "('न', 'क'): 46282\n",
      "('स', 'थ'): 41975\n",
      "('ह', 'क'): 40904\n",
      "('ग', 'य'): 39106\n",
      "('ह', '.'): 38982\n",
      "('क', 'स'): 37681\n",
      "('क', ''): 37031\n",
      "('रह', 'ह'): 36458\n",
      "('ह', ','): 34402\n",
      "('य', 'ह'): 34116\n",
      "('', 'दन'): 32920\n",
      "('क', 'पर'): 31494\n",
      "('थ', '।'): 29454\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of token\n",
    "\n",
    "token_counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjmVAdO82urR"
   },
   "source": [
    "Volume = 2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 6849,
     "status": "ok",
     "timestamp": 1708155300507,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "MMt9XDs42uLm"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the IndicBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Adjust the max_model_input_sizes parameter to limit the vocabulary size\n",
    "tokenizer.max_model_input_sizes = {\"ai4bharat/indic-bert\": 2000}\n",
    "\n",
    "# Path to your text file\n",
    "file_path = \"hi_100_10k.txt\"\n",
    "\n",
    "# Read text from the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize text using the IndicBERT tokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "tokens = [string.replace(\"▁\", \"\") for string in tokens]\n",
    "\n",
    "# Remove numeric strings from the array\n",
    "tokens = [token for token in tokens if not re.match(r'^[0-9]+$', token)]\n",
    "\n",
    "\n",
    "# Print the tokens\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2506,
     "status": "ok",
     "timestamp": 1708155307347,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "N5-PGt1P6n2l",
    "outputId": "7b82b717-3623-4b9c-9489-48f49b9b30bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 19017320\n",
      "क् -> 2220998\n",
      "र् -> 2115732\n",
      "न् -> 1335136\n",
      "स् -> 1283598\n",
      "ह् -> 1133109\n",
      "म् -> 1053158\n",
      "त् -> 980008\n",
      "ल् -> 919858\n",
      "प् -> 805828\n",
      "य् -> 753163\n",
      "व् -> 624678\n",
      "द् -> 607586\n",
      "ज् -> 556551\n",
      "ब् -> 524575\n",
      "ग् -> 479916\n",
      "ट् -> 318582\n",
      "श् -> 297460\n",
      "ए -> 286303\n",
      "।् -> 276879\n",
      "First 20 most occurring bigrams:\n",
      "क्अ -> 2220998\n",
      "र्अ -> 2115732\n",
      "अर् -> 1539413\n",
      "न्अ -> 1335136\n",
      "स्अ -> 1283598\n",
      "ह्अ -> 1133109\n",
      "म्अ -> 1053158\n",
      "त्अ -> 980008\n",
      "ल्अ -> 919858\n",
      "प्अ -> 805828\n",
      "य्अ -> 753163\n",
      "अन् -> 694189\n",
      "व्अ -> 624678\n",
      "द्अ -> 607586\n",
      "ज्अ -> 556551\n",
      "अल् -> 526114\n",
      "ब्अ -> 524575\n",
      "ग्अ -> 479916\n",
      "अत् -> 422462\n",
      "अह् -> 410498\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1136,
     "status": "ok",
     "timestamp": 1708155312513,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "9QpjuA9N6sa4",
    "outputId": "6977b9d0-6493-48e9-d408-1c9a9cad6c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "क -> 2220998\n",
      "र -> 2115732\n",
      "न -> 1335136\n",
      "स -> 1283598\n",
      "ह -> 1133109\n",
      "म -> 1053158\n",
      "त -> 980008\n",
      "ल -> 919858\n",
      "प -> 805828\n",
      "य -> 753163\n",
      "व -> 624678\n",
      "द -> 607586\n",
      "ज -> 556551\n",
      "ब -> 524575\n",
      "ग -> 479916\n",
      "ट -> 318582\n",
      "श -> 297460\n",
      "ए -> 286303\n",
      "च -> 272528\n",
      "अ -> 253182\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 331417\n",
      "पर -> 311463\n",
      "और -> 115918\n",
      "इस -> 100938\n",
      "तर -> 94880\n",
      "रह -> 93236\n",
      "वर -> 91160\n",
      "दर -> 81144\n",
      "रत -> 81053\n",
      "सम -> 73028\n",
      "सर -> 69816\n",
      "गर -> 69613\n",
      "जन -> 67614\n",
      "लग -> 63925\n",
      "मन -> 62257\n",
      "एक -> 60679\n",
      "बर -> 59271\n",
      "पन -> 58530\n",
      "उन -> 57239\n",
      "मर -> 55870\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllable\n",
    "\n",
    "syllable_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1708155316105,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "MqEMUv4A6z7p",
    "outputId": "5898620e-ea8b-418e-f01d-709baa2efcca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "क: 1148430\n",
      "ह: 528771\n",
      "य: 451165\n",
      "न: 415566\n",
      "स: 388035\n",
      "म: 358981\n",
      "त: 314370\n",
      "पर: 278825\n",
      "।: 274128\n",
      ": 255196\n",
      "कर: 208176\n",
      "द: 187335\n",
      ",: 181924\n",
      "ल: 171849\n",
      "व: 164840\n",
      "ग: 158211\n",
      "ज: 156321\n",
      "र: 146549\n",
      "थ: 139596\n",
      "ए: 136758\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('ह', '।'): 129657\n",
      "('न', 'ह'): 61184\n",
      "('त', 'ह'): 59911\n",
      "('क', 'य'): 58547\n",
      "('य', 'क'): 56876\n",
      "('ल', 'ए'): 54172\n",
      "('क', 'ल'): 48659\n",
      "('न', 'क'): 46282\n",
      "('स', 'थ'): 41975\n",
      "('ह', 'क'): 40904\n",
      "('ग', 'य'): 39106\n",
      "('ह', '.'): 38982\n",
      "('क', 'स'): 37681\n",
      "('क', ''): 37031\n",
      "('रह', 'ह'): 36458\n",
      "('ह', ','): 34402\n",
      "('य', 'ह'): 34116\n",
      "('', 'दन'): 32920\n",
      "('क', 'पर'): 31494\n",
      "('थ', '।'): 29454\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of token\n",
    "\n",
    "token_counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA9FwDPW1jIn"
   },
   "source": [
    "**WHITE SPACE TOKENIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1708155470747,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "LHmm6CEo1nCp"
   },
   "outputs": [],
   "source": [
    "# Open the file and read its contents\n",
    "with open(\"hi_100_10k.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize the text using white space tokenization\n",
    "tokens = text.split()\n",
    "\n",
    "# Remove numeric strings from the array\n",
    "tokens = [token for token in tokens if not re.match(r'^[0-9]+$', token)]\n",
    "\n",
    "# Print the updated array\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2313,
     "status": "ok",
     "timestamp": 1708155476178,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "gaFj2ttX2HQ-",
    "outputId": "e5cf2cd1-0c10-41af-c1a4-96db6b7ff675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ -> 8161727\n",
      "आ -> 2990017\n",
      "ए -> 2315978\n",
      "क् -> 2219861\n",
      "र् -> 2115469\n",
      "ई -> 1459998\n",
      "इ -> 1432475\n",
      "न् -> 1334352\n",
      "स् -> 1283598\n",
      "ह् -> 1133109\n",
      "म् -> 1053158\n",
      "त् -> 980008\n",
      "ल् -> 919858\n",
      "ओ -> 896387\n",
      "प् -> 805828\n",
      "य् -> 752754\n",
      "व् -> 624678\n",
      "द् -> 607586\n",
      "उ -> 586868\n",
      "ज् -> 551320\n",
      "First 20 most occurring bigrams:\n",
      "र्अ -> 1173256\n",
      "अर् -> 795345\n",
      "क्अ -> 619371\n",
      "स्अ -> 518828\n",
      "न्अ -> 516056\n",
      "अन् -> 441562\n",
      "क्ए -> 407120\n",
      "प्अ -> 405643\n",
      "अह् -> 394624\n",
      "आर् -> 368674\n",
      "अक् -> 365504\n",
      "त्अ -> 354049\n",
      "ल्अ -> 333301\n",
      "न्ए -> 328943\n",
      "म्अ -> 324946\n",
      "अत् -> 319246\n",
      "क्आ -> 314299\n",
      "य्आ -> 297759\n",
      "ह्ऐ -> 297198\n",
      "म्ए -> 296589\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of characters\n",
    "\n",
    "character_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1133,
     "status": "ok",
     "timestamp": 1708155490311,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "sNos_WeT2QcE",
    "outputId": "0fc76b66-1861-47d1-ed25-8a0ca0bd7d68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "र -> 1006954\n",
      "क -> 607180\n",
      "न -> 505279\n",
      "स -> 489944\n",
      "के -> 404995\n",
      "प -> 391607\n",
      "ल -> 328508\n",
      "ने -> 328091\n",
      "त -> 308900\n",
      "का -> 307944\n",
      "है -> 297018\n",
      "म -> 294134\n",
      "मे -> 292699\n",
      "ह -> 280526\n",
      "ए -> 270868\n",
      "ब -> 247105\n",
      "अ -> 235816\n",
      "की -> 235314\n",
      "ग -> 226095\n",
      "या -> 225519\n",
      "\n",
      "First 20 most occurring bigrams:\n",
      "कर -> 160083\n",
      "और -> 115599\n",
      "पर -> 99658\n",
      "इस -> 82680\n",
      "एक -> 54327\n",
      "लिए -> 53999\n",
      "नही -> 49366\n",
      "अप -> 45146\n",
      "कार -> 38961\n",
      "किया -> 37433\n",
      "है. -> 37339\n",
      "रने -> 34495\n",
      "कहा -> 33135\n",
      "यह -> 31421\n",
      "गया -> 30280\n",
      "सर -> 30072\n",
      "है, -> 29020\n",
      "उन -> 28938\n",
      "आप -> 28848\n",
      "साथ -> 27730\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of syllable\n",
    "\n",
    "syllable_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1708155494998,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "d2BrUu_U2bqA",
    "outputId": "c8df925f-490a-418e-8dd4-e52705b9ffde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 unigram frequencies of tokens:\n",
      "के: 316314\n",
      "में: 237426\n",
      "की: 189769\n",
      "को: 145266\n",
      "से: 136979\n",
      "और: 114563\n",
      "का: 109743\n",
      "ने: 102378\n",
      "पर: 88560\n",
      "है।: 88260\n",
      "कि: 76741\n",
      "है: 72656\n",
      "भी: 63920\n",
      "एक: 49279\n",
      "लिए: 47955\n",
      "इस: 47558\n",
      "कर: 44563\n",
      "नहीं: 44168\n",
      "ही: 40104\n",
      "तो: 33469\n",
      "Top 20 bigram frequencies of tokens:\n",
      "('के', 'लिए'): 42448\n",
      "('है', 'कि'): 24816\n",
      "('के', 'साथ'): 17075\n",
      "('कहा', 'कि'): 15922\n",
      "('के', 'बाद'): 14163\n",
      "('है', 'और'): 10107\n",
      "('ने', 'कहा'): 9098\n",
      "('करने', 'के'): 8826\n",
      "('बताया', 'कि'): 6392\n",
      "('को', 'लेकर'): 5978\n",
      "('.', '.'): 5702\n",
      "('गया', 'है।'): 5678\n",
      "('रहा', 'है।'): 5483\n",
      "('के', 'खिलाफ'): 5312\n",
      "('के', 'दौरान'): 5162\n",
      "('के', 'बीच'): 5117\n",
      "('बारे', 'में'): 5059\n",
      "('करते', 'हुए'): 4817\n",
      "('रहे', 'हैं।'): 4702\n",
      "('में', 'भी'): 4687\n"
     ]
    }
   ],
   "source": [
    "#counting frequencies of token\n",
    "\n",
    "token_counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0dnJ69TJ7Wb"
   },
   "source": [
    "**RECALL, PRECISION, F-SCORE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708155498155,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "tlzJHL_7KAhz"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as sp\n",
    "sentences = [\n",
    "    [\"सभी वित्तीय योजनाएं खासी जटिल और ऊबाऊ हैं।\"],\n",
    "    [\"उन्होंने बताया कि आरोपियों की गिरफ्तारी के पुलिस लगातार छापेमारी कर रही है।\"],\n",
    "    [\"जिससे उनकी गाड़ी दूर तक पलटती हुई चली गई और स्कार्पियो चालक गाड़ी को घटनास्थल से लेकर फरार हो गया।\"],\n",
    "    [\"सभी लगातार निजी गैर स्कूल और पूर्व-अल्पसंख्यक इस वेबसाइट पेज पर बीएओयू परीक्षा दिनांक पत्र 201 9 डाउनलोड करने के लिए तैयार होंगे।\"],\n",
    "    [\"टाइटलर और वर्मा पर 2009 में तत्कालीन प्रधानमंत्री मंत्री मनमोहन सिंह को संबोधित एक फर्जी पत्र के कथित इस्तेमाल का मुकदमा चल रहा है।\"],\n",
    "    [\"लुटेरे उनसे करीब चार हजार रुपये की नकदी व मोबाइल फोन लूट कर\"],\n",
    "    [\"सबसे कम महिला साक्षरता वाले इन जिलों में विशेष अभियान चलाया जाएगा।\"],\n",
    "    [\"कॉलेज की बीएससी तृतीय वर्ष की छात्रा रितिका, बीएससी द्वितीय वर्ष की छात्राओं मानसी, मधु, काजल गिल, मीनू और बीए द्वितीय वर्ष की टीनू दहिया, पूजा, प्रियंका, नेहा दहिया, बीएससी द्वितीय वर्ष (मैथ ऑर्नस) की प्रिया ने कांस्य पदक हासिल किया।\"],\n",
    "    [\"पश्चिमी UP के छात्रों को JNU-जामिया में 10% आरक्षण दे दो, सबका इलाज कर देंगे: संजीव बालियान\"],\n",
    "    [\"सरकार की इस घोषणा के बाद राज्य के लोगों में भय का माहौल बन गया है।\"],\n",
    "    [\"विशिष्ट और लंबी-पूंछ खोज शब्दों के मुकाबले दूसरों की तुलना में अधिक आशय है और छोटे या हाल ही में स्थापित व्यवसायों के लिए रैंकिंग बूस्टर के रूप में सेवा कर सकते हैं।\"],\n",
    "    [\"अंतरराष्ट्रीय एजेंसी ने कुल 27 मानकों के पालन करने का निर्देश दिया है जिसमें से मई, 2019 तक सिर्फ पांच मानकों का पालन ही पाकिस्तान कर पाया है।\"],\n",
    "    [\"सकारात्मक चली सदन की कार्यवाही\"],\n",
    "    [\"यह फ़ाइल फ़ोटो पूर्व फ़िलिस्तीनी प्रोफ़ेसर अब्दुल हलीम अलअश्क़र की अमरीका के वर्जीनिया स्थित घर की है\"],\n",
    "    ['क्या ऐसा हो सकता है की योग के कोई भी दुष्प्रभाव ना हो|'],\n",
    "    [\"मैंने कभी भी तुम्हारा मित्र के अलावा किसी और तरीके से स्पर्श किया है?\"],\n",
    "    [\"उन्होंने कहा है कि उनका किसी महिला से कोई संबंध नहीं है।\"],\n",
    "    [\"लेकिन राज्यसभा के सूत्रों ने दावा किया कि ऑडिट ऊंचाई वाले इलाकों में भारतीय सेना की स्थिति को उजागर करती है।\"],\n",
    "    [\"आनी  — आनी खंड के कराणा पंचायत के जाबो क्षेत्र के लोग पिछले करीब एक हफ्ते से पैदल अपने घरों को जाने के लिए मजबूर हैं।\"],\n",
    "    [\"जींद, 24 फरवरी (हप्र) शहर के एकलव्य स्टेडियम के गेट के सामने रविवार सायं 4 बाइकों पर सवार 8-10 युवकों ने अर्बन एस्टेट के अक्षय (18) की गोली मारकर हत्या कर दी।\"],\n",
    "    [\"जोधपुर संभाग में पांच लोगों को सांप काटने से घायल होने पर यहां जोधपुर के मथुरादास माथुर अस्पताल में रैफर किया गया है।\"],\n",
    "    [\"वे बताती हैं, ‘पिछले साल चैत का महीना चल रहा था, सोमवार का दिन था.\"],\n",
    "    [\"154 यात्रियों को लेकर हांगकांग से दिल्ली आ रहे एयर इंडिया के एक विमान के इंजन में खराबी आने के बाद उसे कोलकाता में आपात स्थिति में उतारा गया।\"],\n",
    "    [\"श्री चौहान ने यह जानकारी उद्यान विभाग द्वारा संचालित फालोद्यान फार्म धौलाकुआं में विभाग द्वारा चलाई जा रही गतिविधियों का जायजा लेते हुए दी।\"],\n",
    "    [\"इस आदेश में किसी को ट्रिब्यूनल के समक्ष प्रस्तुत करने से पहले पुलिस द्वारा पूर्व जांच करने का जिक्र नहीं किया गया.\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "truth = [['सभी वित्तीय योजनाएं', 'खासी', 'जटिल और ऊबाऊ हैं।'],\n",
    "         ['उन्होंने', 'बताया कि', 'आरोपियों की', 'गिरफ्तारी के', 'पुलिस', 'लगातार छापेमारी कर रही है।'],\n",
    "         ['जिससे', 'उनकी गाड़ी', 'दूर तक', 'पलटती हुई चली गई', 'और', 'स्कार्पियो चालक', 'गाड़ी को', 'घटनास्थल से', 'लेकर फरार हो गया।'],\n",
    "         ['सभी लगातार', 'निजी गैर स्कूल और पूर्व-अल्पसंख्यक', 'इस वेबसाइट पेज पर', 'बीएओयू परीक्षा दिनांक पत्र 2019', 'डाउनलोड करने के लिए', 'तैयार होंगे।'],\n",
    "         ['टाइटलर और वर्मा पर', '2009 में', 'तत्कालीन प्रधानमंत्री मंत्री मनमोहन सिंह को', 'संबोधित', 'एक फर्जी पत्र के', 'कथित इस्तेमाल का', 'मुकदमा चल रहा है।'],\n",
    "         ['लुटेरे', 'उनसे', 'करीब चार हजार रुपये की', 'नकदी व', 'मोबाइल फोन लूट कर'],\n",
    "         ['सबसे कम महिला साक्षरता वाले', 'इन जिलों में', 'विशेष अभियान', 'चलाया जाएगा।'],\n",
    "         ['कॉलेज की बीएससी तृतीय वर्ष की छात्रा रितिका', 'बीएससी द्वितीय वर्ष की छात्राओं मानसी', 'मधु', 'काजल गिल', 'मीनू और बीए द्वितीय वर्ष की टीनू दहिया', 'पूजा', 'प्रियंका', 'नेहा दहिया', 'बीएससी द्वितीय वर्ष (मैथ ऑर्नस) की प्रिया', 'कांस्य पदक हासिल किया।'],\n",
    "         ['पश्चिमी UP के छात्रों को', 'JNU-जामिया में', '10% आरक्षण दे दो', 'सबका इलाज कर देंगे:', 'संजीव बालियान'],\n",
    "         ['सरकार की', 'इस घोषणा के बाद', 'राज्य के लोगों में', 'भय का माहौल बन गया है।'],\n",
    "         ['विशिष्ट और लंबी-पूंछ खोज शब्दों के मुकाबले', 'दूसरों की तुलना में', 'अधिक आशय है', 'और', 'छोटे या हाल ही में स्थापित व्यवसायों के लिए', 'रैंकिंग बूस्टर के रूप में', 'सेवा कर सकते हैं।'],\n",
    "         ['अंतरराष्ट्रीय एजेंसी ने', 'कुल 27 मानकों के', 'पालन करने का', 'निर्देश दिया है', 'जिसमें से', 'मई', '2019 तक', 'सिर्फ पांच मानकों का पालन ही', 'पाकिस्तान कर पाया है।'],\n",
    "         ['सकारात्मक चली', 'सदन की कार्यवाही'],\n",
    "         ['यह फ़ाइल फ़ोटो', 'पूर्व फ़िलिस्तीनी प्रोफ़ेसर अब्दुल हलीम अलअश्क़र की', 'अमरीका के वर्जीनिया स्थित घर की है'],\n",
    "         ['क्या ऐसा हो सकता है', 'की', 'योग के', 'कोई भी दुष्प्रभाव', 'ना हो|'],\n",
    "         ['मैंने', 'कभी भी', 'तुम्हारा', 'मित्र के अलावा', 'किसी और तरीके', 'से स्पर्श किया है?'],\n",
    "         ['उन्होंने', 'कहा है', 'कि', 'उनका', 'किसी महिला से', 'कोई संबंध नहीं है।'],\n",
    "         ['लेकिन', 'राज्यसभा के सूत्रों ने', 'दावा किया', 'कि', 'ऑडिट', 'ऊंचाई वाले इलाकों में', 'भारतीय सेना की स्थिति को', 'उजागर करती है।'],\n",
    "         ['आनी  — आनी खंड के कराणा पंचायत के जाबो क्षेत्र के लोग', 'पिछले करीब एक हफ्ते से', 'पैदल', 'अपने घरों को जाने के लिए', 'मजबूर हैं।'],\n",
    "         ['जींद, 24 फरवरी (हप्र)', 'शहर के एकलव्य स्टेडियम के गेट के सामने', 'रविवार सायं 4 बाइकों पर सवार', '8-10 युवकों ने', 'अर्बन एस्टेट के अक्षय (18) की', 'गोली मारकर हत्या कर दी।'],\n",
    "         ['जोधपुर संभाग में', 'पांच लोगों को', 'सांप काटने से', 'घायल होने पर', 'यहां जोधपुर के मथुरादास माथुर अस्पताल में', 'रैफर किया गया है।'],\n",
    "         ['वे बताती हैं', '‘पिछले साल', 'चैत का महीना चल रहा था', 'सोमवार का दिन था.'],\n",
    "         ['154 यात्रियों को लेकर', 'हांगकांग से दिल्ली आ रहे', 'एयर इंडिया के एक विमान के इंजन में खराबी आने के बाद', 'उसे कोलकाता में', 'आपात स्थिति में', 'उतारा गया।'],\n",
    "         ['श्री चौहान ने', 'यह जानकारी', 'उद्यान विभाग द्वारा संचालित फालोद्यान फार्म धौलाकुआं में', 'विभाग द्वारा चलाई जा रही', 'गतिविधियों का', 'जायजा लेते हुए', 'दी।'],\n",
    "         ['इस आदेश में', 'किसी को', 'ट्रिब्यूनल के समक्ष प्रस्तुत करने से पहले', 'पुलिस द्वारा', 'पूर्व जांच करने का', 'जिक्र नहीं किया गया']\n",
    "         ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(ground_truth, model_predictions):\n",
    "    if len(ground_truth) != len(model_predictions):\n",
    "        raise ValueError(\"Number of sentences in ground truth and model predictions arrays must be the same.\")\n",
    "    true_positives = 0\n",
    "    total_tokens = 0\n",
    "    total_ground_truth_tokens = 0\n",
    "\n",
    "    for i in range(len(ground_truth)):\n",
    "        \n",
    "        total_ground_truth_tokens += len(ground_truth[i])\n",
    "\n",
    "        for j in range(len(ground_truth[i])):\n",
    "            total_tokens += 1\n",
    "            if ground_truth[i][j] in model_predictions[i]:\n",
    "                true_positives += 1\n",
    "\n",
    "    precision = true_positives / total_tokens if total_tokens > 0 else 0\n",
    "    recall = true_positives / total_ground_truth_tokens if total_ground_truth_tokens > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    accuracy = true_positives / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1708155501415,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "enoT28gNKHU7"
   },
   "outputs": [],
   "source": [
    "# def parameter_calculator(tokenized_sentences):\n",
    "#     truePositives = 0\n",
    "#     falsePositives = 0\n",
    "#     falseNegatives = 0\n",
    "#     trueNegatives = 0\n",
    "\n",
    "#     # Iterate over each sentence pair\n",
    "#     for truth_sentence, sentence in zip(truth, tokenized_sentences):\n",
    "#         # Convert each sentence to a set for faster lookup\n",
    "#         sentence_set = set(sentence)\n",
    "#         truth_set = set(truth_sentence)\n",
    "\n",
    "#         # Calculate true positives\n",
    "#         truePositives += len(sentence_set.intersection(truth_set))\n",
    "\n",
    "#         # Calculate false positives\n",
    "#         falsePositives += len(sentence_set.difference(truth_set))\n",
    "\n",
    "#         # Calculate false negatives\n",
    "#         falseNegatives += len(truth_set.difference(sentence_set))\n",
    "\n",
    "#         # Calculate true negatives\n",
    "#         trueNegatives += len(set(truth_set).union(set(sentence_set))) - (len(set(truth_set)) + len(set(sentence_set)) - len(set(truth_set).intersection(set(sentence_set))))\n",
    "\n",
    "#     # Output the results\n",
    "#     print(\"True Positives:\", truePositives)\n",
    "#     print(\"False Positives:\", falsePositives)\n",
    "#     print(\"False Negatives:\", falseNegatives)\n",
    "#     print(\"True Negatives:\", trueNegatives)\n",
    "#     return truePositives,trueNegatives, falsePositives, falseNegatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1708155506772,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "5OLnisr8KKaz"
   },
   "outputs": [],
   "source": [
    "# def calculate_metrics(tp, tn, fp, fn):\n",
    "#     # Calculate precision\n",
    "#     precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "#     # Calculate recall\n",
    "#     recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "#     # Calculate F1-score\n",
    "#     fscore = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "#     print(\"Precision:\", precision)\n",
    "#     print(\"Recall:\", recall)\n",
    "#     print(\"F1-score:\", fscore)\n",
    "#     return precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1708155518158,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "tsDvL2-SKO8t",
    "outputId": "53d870a9-10b5-4d17-8cb2-c80d18fe05fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Precision: 0.07586206896551724\n",
      "Recall: 0.07586206896551724\n",
      "F1-score: 0.07586206896551724\n",
      "Accuracy: 0.07586206896551724\n"
     ]
    }
   ],
   "source": [
    "#UNIGRAM TOKENIZER, 1K\n",
    "\n",
    "s = sp.SentencePieceProcessor()\n",
    "s.load(\"unigram_1k.model\")\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "    tokens = s.encode(sentence[0], out_type=str)\n",
    "    tokens = [string.replace(\"▁\", \"\") for string in tokens]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "# print(len(tokenized_sentences))\n",
    "\n",
    "precision, recall, f1_score, accuracy = calculate_metrics(truth, tokenized_sentences)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1708155550643,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "8LOtc0QrKSjc",
    "outputId": "f27e5205-0515-4e6f-ef1e-0dff8ec01366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.11724137931034483\n",
      "Recall: 0.11724137931034483\n",
      "F1-score: 0.11724137931034483\n",
      "Accuracy: 0.11724137931034483\n"
     ]
    }
   ],
   "source": [
    "#UNIGRAM TOKENIZER, 2K\n",
    "\n",
    "s = sp.SentencePieceProcessor()\n",
    "s.load(\"unigram_2k.model\")\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "    tokens = s.encode(sentence[0], out_type=str)\n",
    "    tokens = [string.replace(\"▁\", \"\") for string in tokens]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "precision, recall, f1_score, accuracy = calculate_metrics(truth, tokenized_sentences)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1708155610835,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "MkXek5KRKW-c",
    "outputId": "1b4bb31e-082a-4379-b174-db707c7da9bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.06896551724137931\n",
      "Recall: 0.06896551724137931\n",
      "F1-score: 0.06896551724137931\n",
      "Accuracy: 0.06896551724137931\n"
     ]
    }
   ],
   "source": [
    "#BPE TOKENIZER, 1K\n",
    "\n",
    "s = sp.SentencePieceProcessor()\n",
    "s.load(\"bpe_1k.model\")\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "    tokens = s.encode(sentence[0], out_type=str)\n",
    "    tokens = [string.replace(\"▁\", \"\") for string in tokens]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "precision, recall, f1_score, accuracy = calculate_metrics(truth, tokenized_sentences)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1708155627785,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "HgeJnU2dKb4t",
    "outputId": "f156e06c-42e4-4ecf-f010-f0d16940b9a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.08275862068965517\n",
      "Recall: 0.08275862068965517\n",
      "F1-score: 0.08275862068965517\n",
      "Accuracy: 0.08275862068965517\n"
     ]
    }
   ],
   "source": [
    "#BPE TOKENIZER,2K\n",
    "\n",
    "s = sp.SentencePieceProcessor()\n",
    "s.load(\"bpe_2k.model\")\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "    tokens = s.encode(sentence[0], out_type=str)\n",
    "    tokens = [string.replace(\"▁\", \"\") for string in tokens]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "precision, recall, f1_score, accuracy = calculate_metrics(truth, tokenized_sentences)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1708156708818,
     "user": {
      "displayName": "Priyanshu Jha",
      "userId": "08056705956897613409"
     },
     "user_tz": -330
    },
    "id": "jiTAIUekKeiV",
    "outputId": "9b5ca68b-b636-4a2d-a5f8-b47603230d38"
   },
   "outputs": [],
   "source": [
    "#mBERT, max_length=1000\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_basic_tokenize=False, max_length=1000)\n",
    "\n",
    "# Tokenize each sentence in the array of arrays\n",
    "tokenized_sentences= []\n",
    "for array in sentences:\n",
    "    arr = []\n",
    "    for sentence in array:\n",
    "        # Tokenize the sentence using the BERT tokenizer\n",
    "        sen= tokenizer.tokenize(sentence)\n",
    "        arr.append(sen)\n",
    "    tokenized_sentences.append(arr)\n",
    "# Flatten the tokenized array of arrays into a single 2D array\n",
    "flattened_tokenized_array = [sentence for array in tokenized_sentences for sentence in array]\n",
    "\n",
    "# Remove '##' prefix from tokens\n",
    "flattened_tokenized_array=[[word.replace('##', '') for word in sublist] for sublist in flattened_tokenized_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "PxSH_WWeKhe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.08275862068965517\n",
      "Recall: 0.08275862068965517\n",
      "F1-score: 0.08275862068965517\n",
      "Accuracy: 0.08275862068965517\n"
     ]
    }
   ],
   "source": [
    "#mBERT TOKENIZER,1K\n",
    "\n",
    "precision, recall, f1_score, accuracy = calculate_metrics(truth, flattened_tokenized_array)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "-wvw7DdsKj7V"
   },
   "outputs": [],
   "source": [
    "#mBERT, max_length=2000\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", max_length=2000)\n",
    "\n",
    "# Tokenize each sentence in the array of arrays\n",
    "tokenized_sentences= []\n",
    "for array in sentences:\n",
    "    arr = []\n",
    "    for sentence in array:\n",
    "        # Tokenize the sentence using the BERT tokenizer\n",
    "        sen= tokenizer.tokenize(sentence)\n",
    "        arr.append(sen)\n",
    "    tokenized_sentences.append(arr)\n",
    "# Flatten the tokenized array of arrays into a single 2D array\n",
    "flattened_tokenized_array = [sentence for array in tokenized_sentences for sentence in array]\n",
    "\n",
    "\n",
    "\n",
    "# Remove '##' prefix from tokens\n",
    "flattened_tokenized_array=[[word.replace('##', '') for word in sublist] for sublist in flattened_tokenized_array]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "zc82wkdQKmiO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.08275862068965517\n",
      "Recall: 0.08275862068965517\n",
      "F1-score: 0.08275862068965517\n",
      "Accuracy: 0.08275862068965517\n"
     ]
    }
   ],
   "source": [
    "#mBERT TOKENIZER,2K\n",
    "\n",
    "precision, recall, f1_score, accuracy = calculate_metrics(truth, flattened_tokenized_array)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "VyloRNrzKpMe"
   },
   "outputs": [],
   "source": [
    "#indicBERT maxLength 1000\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the IndicBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Adjust the max_model_input_sizes parameter to limit the vocabulary size\n",
    "tokenizer.max_model_input_sizes = {\"ai4bharat/indic-bert\": 1000}\n",
    "\n",
    "# Tokenize each sentence in the array of arrays\n",
    "tokenized_sentences = [tokenizer.tokenize(sentence) for sentence_array in sentences for sentence in sentence_array]\n",
    "\n",
    "tokenized_sentences=[[word.replace('▁', '') for word in sublist] for sublist in tokenized_sentences]\n",
    "\n",
    "# print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.020689655172413793\n",
      "Recall: 0.020689655172413793\n",
      "F1-score: 0.020689655172413793\n",
      "Accuracy: 0.020689655172413793\n"
     ]
    }
   ],
   "source": [
    "#indicBERT 1K\n",
    "\n",
    "precision, recall, f1_score, accuracy = calculate_metrics(truth, tokenized_sentences)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicBERT maxLength 2000\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the IndicBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Adjust the max_model_input_sizes parameter to limit the vocabulary size\n",
    "tokenizer.max_model_input_sizes = {\"ai4bharat/indic-bert\": 2000}\n",
    "\n",
    "# Tokenize each sentence in the array of arrays\n",
    "tokenized_sentences = [tokenizer.tokenize(sentence) for sentence_array in sentences for sentence in sentence_array]\n",
    "\n",
    "tokenized_sentences=[[word.replace('▁', '') for word in sublist] for sublist in tokenized_sentences]\n",
    "# print(tokenized_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.020689655172413793\n",
      "Recall: 0.020689655172413793\n",
      "F1-score: 0.020689655172413793\n",
      "Accuracy: 0.020689655172413793\n"
     ]
    }
   ],
   "source": [
    "#indicBERT 2K\n",
    "\n",
    "precision, recall, f1_score, accuracy = calculate_metrics(truth, tokenized_sentences)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00b5b59737804b8b8e11b25506e1aa93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ca515dfe5534b86af3824035f7f6239": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b2957007e94420c8daaefd1e55dc229",
      "max": 507,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c438d69fd1904c979146ac383efa543f",
      "value": 507
     }
    },
    "1d95cf017efa466692bebe5e4d9a79f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "222d6a5624df4859855de6c3cfe7f834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c0bafabeb6d4ca5850baa4e14387741",
      "placeholder": "​",
      "style": "IPY_MODEL_1d95cf017efa466692bebe5e4d9a79f9",
      "value": " 507/507 [00:00&lt;00:00, 30.8kB/s]"
     }
    },
    "27a718e90115457c91df941a5851c567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91c8d5a0154b4b03b28a04c9676a0e35",
      "max": 5646064,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abe393450545466a8918234d4dc168d8",
      "value": 5646064
     }
    },
    "454a4782f1d44b179fa3f8ac69c77328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "488a789c9bde41a3a9d01440f67decc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af8a40064d654ef99fa7b741e475b43e",
       "IPY_MODEL_1ca515dfe5534b86af3824035f7f6239",
       "IPY_MODEL_222d6a5624df4859855de6c3cfe7f834"
      ],
      "layout": "IPY_MODEL_454a4782f1d44b179fa3f8ac69c77328"
     }
    },
    "5eb934c2341341a08f2e9a8972b21249": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89b110c7d1924ffe8cc643be8f02b8fb",
      "placeholder": "​",
      "style": "IPY_MODEL_d5d00fdf0acc454595062d61ad9e0b39",
      "value": " 5.65M/5.65M [00:00&lt;00:00, 26.8MB/s]"
     }
    },
    "63883d5281ab469c9a1ba43bbbd5b73e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68b8d34b478a4a5ab3558a7434cb7459": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ae36ca536c940139a04d95e84ac15d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00b5b59737804b8b8e11b25506e1aa93",
      "placeholder": "​",
      "style": "IPY_MODEL_f3f6c3122c1c4f33831356b2ba2d2d20",
      "value": "spiece.model: 100%"
     }
    },
    "7b2957007e94420c8daaefd1e55dc229": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89b110c7d1924ffe8cc643be8f02b8fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c0bafabeb6d4ca5850baa4e14387741": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91c8d5a0154b4b03b28a04c9676a0e35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abe393450545466a8918234d4dc168d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af8a40064d654ef99fa7b741e475b43e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68b8d34b478a4a5ab3558a7434cb7459",
      "placeholder": "​",
      "style": "IPY_MODEL_63883d5281ab469c9a1ba43bbbd5b73e",
      "value": "config.json: 100%"
     }
    },
    "c438d69fd1904c979146ac383efa543f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf2a9764fb364d599e58375103a9b5c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5d00fdf0acc454595062d61ad9e0b39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3f6c3122c1c4f33831356b2ba2d2d20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8432510e8ce4bf696310dfc7ebf5267": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ae36ca536c940139a04d95e84ac15d8",
       "IPY_MODEL_27a718e90115457c91df941a5851c567",
       "IPY_MODEL_5eb934c2341341a08f2e9a8972b21249"
      ],
      "layout": "IPY_MODEL_cf2a9764fb364d599e58375103a9b5c1"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
